<!DOCTYPE html>
<html lang="zh-cn" dir="ltr" class="scroll-smooth" data-default-appearance="dark"
  data-auto-appearance="true"><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  
  <meta http-equiv="content-language" content="zh-cn" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta http-equiv="X-UA-Compatible" content="ie=edge" />
  
  <title>Goal-Conditioned RL &middot; 海豚的博客</title>
  <meta name="title" content="Goal-Conditioned RL &middot; 海豚的博客" />
  
  <meta name="description" content="Goal-Conditioned RL" />
  <meta name="keywords" content="rl, " />
  
  
  <link rel="canonical" href="http://localhost:1313/posts/9-gcrl/" />
  
  
  
  
  
  
  
  
  
  
  <link type="text/css" rel="stylesheet" href="/css/main.bundle.min.4e2716d77ddb668a7c0d68157cff2048cbdf59d43ac1a9de02fd1a04ed3047a1829f54a8b18c01b94597aac05a30d1933f930e6c59382c3e25501049c92fc230.css"
    integrity="sha512-TicW133bZop8DWgVfP8gSMvfWdQ6waneAv0aBO0wR6GCn1SosYwBuUWXqsBaMNGTP5MObFk4LD4lUBBJyS/CMA==" />
  
  
  <script type="text/javascript" src="/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js"
    integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj&#43;e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script>
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  <script defer type="text/javascript" id="script-bundle" src="/js/main.bundle.min.a2d78d78672e549fbfc972ece871725b5478ba0b65708dda20cb97ab80a865eae6d247e1b05a4aec6ebbf78647ec3233bad8b2609ed98eee53cd58aa17128bc7.js"
    integrity="sha512-oteNeGcuVJ&#43;/yXLs6HFyW1R4ugtlcI3aIMuXq4CoZerm0kfhsFpK7G6794ZH7DIzutiyYJ7Zju5TzViqFxKLxw==" data-copy="" data-copied=""></script>
  
  
  
  <script src="/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S&#43;Yti0U7QtuZvQ=="></script>
  
  
  
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
  <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  
  <meta property="og:url" content="http://localhost:1313/posts/9-gcrl/">
  <meta property="og:site_name" content="海豚的博客">
  <meta property="og:title" content="Goal-Conditioned RL">
  <meta property="og:description" content="Goal-Conditioned RL">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-11-21T13:11:22+08:00">
    <meta property="article:modified_time" content="2023-11-23T13:11:22+08:00">
    <meta property="article:tag" content="Rl">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Goal-Conditioned RL">
  <meta name="twitter:description" content="Goal-Conditioned RL">

  
  <script type="application/ld+json">
  [{
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Posts",
    "name": "Goal-Conditioned RL",
    "headline": "Goal-Conditioned RL",
    "description": "Goal-Conditioned RL",
    
    "inLanguage": "zh-cn",
    "url" : "http:\/\/localhost:1313\/posts\/9-gcrl\/",
    "author" : {
      "@type": "Person",
      "name": ""
    },
    "copyrightYear": "2023",
    "dateCreated": "2023-11-21T13:11:22\u002b08:00",
    "datePublished": "2023-11-21T13:11:22\u002b08:00",
    
    "dateModified": "2023-11-23T13:11:22\u002b08:00",
    
    "keywords": ["rl"],
    
    "mainEntityOfPage": "true",
    "wordCount": "541"
  }]
  </script>


  
  
  
  
  

<script src="/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js" integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj&#43;KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script>











<link type="text/css" rel="stylesheet" href="/lib/katex/katex.min.68e17230ccd917b97b7a2def38a8108918599d8aa4f580bfb8cce5e13d23e4de43dcaba5f9000553cb2c10d0d1300aabfe5c433a3305ebd752609f0762a63e59.css" integrity="sha512-aOFyMMzZF7l7ei3vOKgQiRhZnYqk9YC/uMzl4T0j5N5D3Kul&#43;QAFU8ssENDRMAqr/lxDOjMF69dSYJ8HYqY&#43;WQ==" />


<script defer src="/lib/katex/katex.min.50f14e69d6a8da7128ae3b63974c544ed377c36d096b5e3750f114e84c89d668b9301d9b0ed3248969aa183aa2e3bc4d2c1e73d5dcb7d462890c45a18d424589.js" integrity="sha512-UPFOadao2nEorjtjl0xUTtN3w20Ja143UPEU6EyJ1mi5MB2bDtMkiWmqGDqi47xNLB5z1dy31GKJDEWhjUJFiQ=="></script>


<script defer src="/lib/katex/auto-render.min.6095714e3aadb63b14ddc4af69346ab12974c1b460654345f8d1860a0b68fcc51b22f68b757433193090bb80afc8965b65cb607e5541d0f5f0f4b2e64d69b9ff.js" integrity="sha512-YJVxTjqttjsU3cSvaTRqsSl0wbRgZUNF&#43;NGGCgto/MUbIvaLdXQzGTCQu4CvyJZbZctgflVB0PXw9LLmTWm5/w=="
  onload="renderMathInElement(document.body);"></script>








































































































































  
  



  
  
  <meta name="theme-color"/>
  
  
</head>
<body
  class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600">
  <div id="the-top" class="absolute flex self-center">
    <a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600"
      href="#main-content"><span
        class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>跳过正文</a>
  </div>
  
  
  <div class="min-h-[148px]"></div>
<div class="fixed inset-x-0 pl-[24px] pr-[24px]" style="z-index:100">
  <div id="menu-blur" class="absolute opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl shadow-2xl"></div>
  <div class="relative max-w-[64rem] ml-auto mr-auto">
    <div style="padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px"
    class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3">
    
    
    
    <div class="flex flex-1 items-center justify-between">
        <nav class="flex space-x-3">

            
            <a href="/" class="text-base font-medium text-gray-500 hover:text-gray-900">海豚的博客</a>
            

        </nav>
        <nav class="hidden md:flex items-center space-x-5 md:ml-12 h-12">

            
            
             
  <div>
  <div class="cursor-pointer flex items-center nested-menu">
    
    <a  href="/posts/"   class="text-base font-medium text-gray-500 hover:text-primary-600 dark:hover:text-primary-400" title="">
      Blog
    </a>
    <span>
      

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


    </span>
  </div>
  <div class="absolute menuhide">
    <div class="pt-2 p-5 mt-2 rounded-xl backdrop-blur shadow-2xl">
      <div class="flex flex-col space-y-3">
        
        <a href="/posts/leetcode/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            leetcode
          </p>
        </a>
        
        <a href="/posts/linux/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
          
          <p class="text-sm font-sm" title="">
            linux
          </p>
        </a>
        
      </div>
    </div>
  </div>
</div>



            
            
  <a href="/resume/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
    
    <p class="text-base font-medium" title="">
        Resume
    </p>
</a>



            
            

            


            
            <button id="search-button" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            


            
            
            <div
                class="ltr:mr-14 rtl:ml-14 flex items-center">
                <button id="appearance-switcher" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400">
                    <div class="flex items-center justify-center dark:hidden">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                    </div>
                    <div class="items-center justify-center hidden dark:flex">
                        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                    </div>
                </button>
            </div>
            

        </nav>
        <div class="flex md:hidden items-center space-x-5 md:ml-12 h-12">

            <span></span>

            


            
            <button id="search-button-mobile" aria-label="Search" class="text-base hover:text-primary-600 dark:hover:text-primary-400"
                title="">
                

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


            </button>
            

            
            
            <button id="appearance-switcher-mobile" aria-label="Dark mode switcher" type="button" class="text-base hover:text-primary-600 dark:hover:text-primary-400" style="margin-right:5px">
                <div class="flex items-center justify-center dark:hidden">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M32 256c0-123.8 100.3-224 223.8-224c11.36 0 29.7 1.668 40.9 3.746c9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3c9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480C132.1 480 32 379.6 32 256z"/></svg>

  </span>


                </div>
                <div class="items-center justify-center hidden dark:flex">
                    

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 159.1c-53.02 0-95.1 42.98-95.1 95.1S202.1 351.1 256 351.1s95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347L446.1 255.1l63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7l-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89L164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6L12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256l-63.15 91.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7l19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109l109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69 0-127.1-57.31-127.1-127.1c0-70.69 57.31-127.1 127.1-127.1s127.1 57.3 127.1 127.1C383.1 326.7 326.7 383.1 256 383.1z"/></svg>

  </span>


                </div>
            </button>
            

        </div>
    </div>
    <div class="-my-2 -mr-2 md:hidden">

        <label id="menu-button" class="block">
            
            <div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400">
                

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M0 96C0 78.33 14.33 64 32 64H416C433.7 64 448 78.33 448 96C448 113.7 433.7 128 416 128H32C14.33 128 0 113.7 0 96zM0 256C0 238.3 14.33 224 32 224H416C433.7 224 448 238.3 448 256C448 273.7 433.7 288 416 288H32C14.33 288 0 273.7 0 256zM416 448H32C14.33 448 0 433.7 0 416C0 398.3 14.33 384 32 384H416C433.7 384 448 398.3 448 416C448 433.7 433.7 448 416 448z"/></svg>

  </span>


            </div>
            <div id="menu-wrapper" style="padding-top:5px;"
                class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50">
                <ul
                    class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl">

                    <li id="menu-close-button">
                        <span
                            class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400">

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>

</span>
                    </li>

                    

                     
  <li class="mt-1">
    <a href="/posts/" class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Blog
        </p>
        <span>
            

  <span class="relative block icon">
    <svg
  xmlns="http://www.w3.org/2000/svg"
  viewBox="0 0 20 20"
  fill="currentColor"
  aria-hidden="true"
>
  <path
    fill-rule="evenodd"
    d="M5.23 7.21a.75.75 0 011.06.02L10 11.168l3.71-3.938a.75.75 0 111.08 1.04l-4.25 4.5a.75.75 0 01-1.08 0l-4.25-4.5a.75.75 0 01.02-1.06z"
    clip-rule="evenodd"
  />
</svg>

  </span>


        </span>
    </a>
</li>

<li class="mt-1">
    <a href="/posts/leetcode/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            leetcode
        </p>
    </a>
</li>

<li class="mt-1">
    <a href="/posts/linux/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-sm font-small" title="">
            linux
        </p>
    </a>
</li>

<li class="mb-2"></li>




                    

                    
  <li class="mt-1">
    <a href="/resume/"  class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400">
        
        <p class="text-bg font-bg" title="">
            Resume
        </p>
    </a>
</li>




                    

                </ul>
                
                

            </div>
        </label>
    </div>
</div>





  </div>
</div>
<script>
  window.addEventListener('scroll', function (e) {
    var scroll = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop || 0;
    var background_blur = document.getElementById('menu-blur');
    background_blur.style.opacity = (scroll / 300);
  });
</script>

  
  <div class="relative flex flex-col grow">
    <main id="main-content" class="grow">
      


<article>
  
  
  
  
  
  
 
  
   
  
 



<div id="hero" class="h-[150px] md:h-[200px]"></div>



    
    <div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom"
    style="background-image:url(/img/xingtong.jpg);">
    


    <div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal">
    </div>
    <div
        class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal">
    </div>
</div>

<div id="background-blur" class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div>
<script>
    window.addEventListener('scroll', function (e) {
        var scroll = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop || 0;
        var background_blur = document.getElementById('background-blur');
        background_blur.style.opacity = (scroll / 300)
    });
</script>

  
  

  <header id="single_header" class="mt-5 max-w-prose">
    
    <h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
      Goal-Conditioned RL
    </h1>
    <div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
      





  
  







  





  



  













<div class="flex flex-row flex-wrap items-center">
  
  
  <time datetime="2023-11-21T13:11:22&#43;08:00">21 November 2023</time><span class="px-2 text-primary-500">&middot;</span><span>541 字</span><span class="px-2 text-primary-500">&middot;</span><span title="预计阅读">3 分钟</span>
  

  
  
</div>








    </div>

    
    
    
    
    

    

    
      
      
        
        
<div class="flex author">
  
  <div class="place-self-center">
    
    
    <div class="text-2xl sm:text-lg">
</div>
  </div>
</div>

      

      
        

      
      <div class="mb-5"></div>
      

    

  </header>
  
  <section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row">
    
    

      <div class="min-w-0 min-h-0 max-w-fit">
        
        


        <div class="article-content max-w-prose mb-20">
          

<blockquote>
<p>传统的强化学习算法都局限在单个目标上，训练好的算法难以完成这个目标外的其他目标。以机械臂操作为例，采用单一策略只能训练抓取同一个位置的物体。对于不同的目标位置，要训练多个策略。另外这类环境通常依赖于人工进行奖励塑形，而设计一个好的奖励函数通常很困难。因此最近的研究更希望使用稀疏的奖励，只有当成功完成任务后才能够获得奖励，在其他的时刻将不会得到奖励。然而智能体在探索过程中，大部分时间内都无法完成任务，导致有价值的样本数目非常少，智能体需要更多的时间进行探索，导致指数级的样本复杂度。</p>
</blockquote>


<h2 class="relative group"><a href="https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v37/schaul15.pdf" target="_blank">Universal Value Function Approximators</a> 
    <div id="universal-value-function-approximatorshttpslinkzhihucomtargethttp3aproceedingsmlrpressv37schaul15pdf" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#universal-value-function-approximatorshttpslinkzhihucomtargethttp3aproceedingsmlrpressv37schaul15pdf" aria-label="锚点">#</a>
    </span>        
    
</h2>
<blockquote>
<p><strong>值函数 $V(s)$</strong> 用于表示状态 s 在实现 agent 总体目标或奖励函数时的效果。从定义来说，值函数量化了某个策略积累某种报酬的能力。从另一个角度思考，值函数量化了一个agent完成某个任务的能力。（摘录自<a href="https://zhuanlan.zhihu.com/p/110468145" target="_blank">UVFA, HER and Inverse RL</a>）</p>
</blockquote>
<p><strong>在一个环境中，任务（目标）可能是不同的，这需要不同的最优值函数去量化完成不同任务的方案</strong>。本文提出了一种单个、一致的值函数逼近器 <strong>UVFA</strong>，在原始的值函数 $V(s)$、$Q(s,a)$ 的基础上增加 goal 作为输入，变成 $V(s,g)$、$Q(s,a,g)$，这样值函数就变成在某一状态（或状态-动作）某一目标下的价值。</p>
<p>另外，本文将目标定义成了最简单的一种情况：<strong>$\mathcal{G} ⊆ \mathcal{S}$</strong>，当状态达到目标时获得一个二进制的奖励 1。</p>
<p>
    <figure>
      <img class="my-0 rounded-md" loading="lazy" src="post_imgs/9-GCRL/methods.png" alt="alt text" />
      
    </figure>
</p>
<p>一般来说，智能体只会看到状态和目标（s，g）的可能组合的一小部分。为了使智能体能够概括到其余组合，上图给出了两种实现 UVFA 的网络结构。</p>
<ol>
<li><strong>Concatenated 结构</strong>. 直接连接 state 和 goal 作为联合输入，可以用 MLP 实现输出到回归目标的映射；</li>
<li><strong>Two-Stream 结构</strong>. 分别将状态 s 和目标 g 输入到各自的网络，映射到 n 维嵌入向量空间，得到对应的两个 embedding vectors，通过某种函数 h 映射成一个标量作为回归输出，文中采用的方法是向量内积。</li>
</ol>


<h3 class="relative group">监督学习 
    <div id="%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>对于 <strong>Two-Stream</strong> 结构，UVFA的训练包括两个步骤：</p>
<ol>
<li>、通过某种方法获取各个状态目标对应的价值（<strong>文中直接给定</strong>），然后引入了一种新的因子分解方法——<a href="https://zhuanlan.zhihu.com/p/677987614" target="_blank">低秩矩阵分解</a>，将回归分解为两个阶段:
<ul>
<li>第一阶段，我们将数据视为一个稀疏的值表，每个观察到的状态 s 对应一行，每个观察到的目标 g 对应一列，并将表分解为状态嵌入 $φ(s)$ 和目标嵌入 $φ(g)$ ，将 $φ(s)$ 表示为 $s$ 行的目标嵌入向量，将 $φ(g)$ 表示为 $g$ 列的目标嵌入向量。</li>
<li>第二阶段，分别使用标准回归技术（例如梯度下降）学习从状态 $s$ 到状态嵌入 $φ(s)$ 以及从目标 $g$ 到目标嵌入 $φ(g)$ 的非线性映射。</li>
</ul>
</li>
<li>将两个嵌入向量通过向量内积整合成一个状态目标值。</li>
</ol>
<p>
    <figure>
      <img class="my-0 rounded-md" loading="lazy" src="post_imgs/9-GCRL/low-rank.png" alt="alt text" />
      
    </figure>
</p>


<h3 class="relative group">强化学习 
    <div id="%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>对于强化学习，文中提供了两种直接从奖励中学习 UVFA 的算法。</p>
<p>第一种算法使用一种 <a href="https://www.ifaamas.org/Proceedings/aamas2011/papers/A6_R70.pdf" target="_blank">Horde of demons</a> 的方式，可以产生不同目标对应的状态目标值，并使用这些值来 seed the table，然后通过低秩分解的方式将其分解为状态对应的嵌入变量和目标对应的嵌入隐变量，从而学习 UVFA $V(s，g; θ)$，该 $V(s，g; θ)$ 能够概括到以前看不见的目标。</p>
<p>
    <figure>
      <img class="my-0 rounded-md" loading="lazy" src="post_imgs/9-GCRL/pseudocode.png" alt="alt text" />
      
    </figure>
</p>
<p>第二种算法采用了<strong>自举</strong>（bootstrapping）的思想，直接从后续状态的 UVFA 值进行更新：
$$
Q(s_t, a_t, g) ← α(rg + γ_g \underset{a&rsquo;}{max} Q(s_{t+1}, a&rsquo;, g))+(1 − α)Q(s_t, a_t, g)
$$</p>


<h3 class="relative group">总结 
    <div id="%E6%80%BB%E7%BB%93" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E6%80%BB%E7%BB%93" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>(摘录自<a href="https://zhuanlan.zhihu.com/p/267524544" target="_blank">分层强化学习survey</a>)</p>
<p>本文提出的统一值函数概念具有迁移意义，同时属于goal-reach范畴，但局限是文中提到比较困难的仍是goal如何选取，但本文并不打算讨论这个问题，因为本文的重点是提供这种考虑目标在内的广义值函数的概念，于是只是简单地选择某些state作为goal，可见goal如何合理选取将是这类分层问题的最大困难。</p>
<p>另一篇写的非常详细的博客：<a href="https://zhuanlan.zhihu.com/p/32309396" target="_blank">对Reinforcement Learning中的小无相功和九阳神功的一些参悟</a></p>


<h2 class="relative group"><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1707.01495" target="_blank">Hindsight Explerience Replay（HER)</a> 
    <div id="hindsight-explerience-replayherhttpslinkzhihucomtargethttps3aarxivorgabs170701495" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#hindsight-explerience-replayherhttpslinkzhihucomtargethttps3aarxivorgabs170701495" aria-label="锚点">#</a>
    </span>        
    
</h2>
<p>HER 是建立在 UVFA 基础上的，原理是回放每个具有任意目标的轨迹。当智能体在一个回合中没有达成目标时，可以使用另外的目标来替代原来的目标，之后根据这个新的目标重新计算奖励，来为智能体提供经验。HER 改善了稀疏奖励下 DRL 的采样效率，可以将该方法与任意的 off-policy 算法结合。</p>
<p>具体地，在 HER 方法下的马尔可夫决策过程表述为一个五元组，其中的状态 $S$ 不仅包含观测 $S_o$，还包括一个期望目标 $S_{dg}$， 且 $S_{dg} \in S_o$。</p>


<h3 class="relative group">实现过程 
    <div id="%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%AE%9E%E7%8E%B0%E8%BF%87%E7%A8%8B" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>实现 HER 算法的关键是构建 replay buffer，包含以下两步：</p>
<ol>
<li>初始化一个固定长度的队列，每次进入一个五元组 $(s_t,a_t,r_t,s_{t+1},g)$，定义初始状态 $s_0$ 和目标 $g$, 智能体根据当前状态 $s_t$ 和目标 $g$ 来采取动作。
奖励的计算如下：</li>
</ol>
<p>$$
r_t \gets r(a_t,s_t||g)
$$</p>
<p>采样出的 $(s_t,a_t,r_t,s_{t+1},g)$ 将被存放到 replay buffer 中。之后，每次都会基于实际目标 $g$ 采样一段完整的智能体经验序列。
2. 使用新的目标 $g&rsquo;$ 重新计算奖励：</p>
<p>$$
r&rsquo; \gets r(a_t,s_t||g&rsquo;)
$$</p>
<p>我们构建新的 transitions $(s_t, a_t, r′, s_{t+1}, g′)$ ，也存放到 replay buffer 中。</p>
<p>
    <figure>
      <img class="my-0 rounded-md" loading="lazy" src="post_imgs/9-GCRL/her.png" alt="alt text" />
      
    </figure>
</p>
<p>文章给出了四种方法去重新获得新的目标 g':</p>
<ul>
<li>final: 将每个回合最后的状态作为新目标</li>
<li><strong>future（效果最好）</strong>: 随机选择 k 个在这个轨迹上并且在当前transition之后的状态作为新目标</li>
<li>episode: 每次选择 k 个在这个轨迹上的状态作为新目标</li>
<li>random: 每次在所有出现过的状态里面选择 k 个状态作为新目标</li>
</ul>
<p>一般我们使用 future 方法，因为它能够更好地利用经验。</p>


<h3 class="relative group">局限性-INNR问题 
    <div id="%E5%B1%80%E9%99%90%E6%80%A7-innr%E9%97%AE%E9%A2%98" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#%E5%B1%80%E9%99%90%E6%80%A7-innr%E9%97%AE%E9%A2%98" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p><em>（这里引用[4]中的文字）</em>。</p>
<p>HER 方法可以通过目标重标记策略，产生足够数量的非负奖励样本，即使智能体实际上没有完成任务。然而，在具有稀疏奖励的复杂顺序物体操作任务中(智能体必须按顺序成功完成每个子任务，以达到期望的最终目标)，由于隐含的 <strong>一致非负奖励（Identical Non-Negative Reward，INNR）</strong> 问题，智能体仍然会受到样本效率低下的困扰。当智能体在探索过程中无法影响已实现目标时，就会出现INNR问题。在这种情况下，智能体无法从相同负奖励的原始样本，或一致非负的虚拟样本中，区分哪个动作更好。换句话说，实际探索的样本都是负样本，目标重标记的虚拟样本都是正样本，因此并不能从这些样本中区分好坏。因此，来自HER的INNR对于策略改进几乎没有帮助，甚至会降低策略的探索能力。这个隐含的INNR问题是HER在标准操作任务中样本效率低下的原因。</p>
<p>例如，在 Push 任务中，智能体必须先接近物体，然后将其推到期望的位置。当智能体在一个episode中未能改变物体的位置时，所有的已实现目标在整个episode 中是相同的。在经验回放过程后，所有的事后目标也将与已实现目标具有相同的值，导致所有的事后样本都是成功的。然而，<strong>这样的成功并不是由智能体的行动造成的</strong>。这些样本对智能体的策略改进没有帮助，甚至阻碍了学习。</p>


<h3 class="relative group">References 
    <div id="references" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#references" aria-label="锚点">#</a>
    </span>        
    
</h3>
<ol>
<li><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Paper.pdf" target="_blank">Hindsight Experience Replay (neurips.cc)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/51357496" target="_blank">【强化学习算法 34】HER - 知乎 (zhihu.com)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/403527126" target="_blank">[强化学习5] HER（Hindsight Experience Replay） - 知乎 (zhihu.com)</a></li>
<li><a href="https://arxiv.org/abs/2208.00843" target="_blank">[2208.00843] Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards (arxiv.org)</a></li>
<li><a href="https://github.com/openai/baselines/tree/master/baselines/her" target="_blank">baselines/baselines/her at master · openai/baselines (github.com)</a></li>
</ol>
<p>openai 使用 her 在机器人操作上的应用研究：

    <figure>
      <img class="my-0 rounded-md" loading="lazy" src="post_imgs/9-GCRL/openai-her.png" alt="alt text" />
      
    </figure>
</p>
<ul>
<li><a href="https://openai.com/research/ingredients-for-robotics-research" target="_blank">ingredients-for-robotics-research</a></li>
<li><a href="https://arxiv.org/pdf/1802.09464.pdf" target="_blank">Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research</a></li>
</ul>


<h2 class="relative group">GCRL with Sub-goal Selection / Generation 
    <div id="gcrl-with-sub-goal-selection--generation" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#gcrl-with-sub-goal-selection--generation" aria-label="锚点">#</a>
    </span>        
    
</h2>
<blockquote>
<p>本文翻译与参考自: <a href="https://arxiv.org/pdf/2201.08299.pdf" target="_blank">Goal-Conditioned Reinforcement Learning: Problems and Solutions</a></p>
</blockquote>
<p>由于难以实现长期目标，可以把目标分割成许多短期目标来帮助智能体到达长期目标。
在期望目标被替换后收集经验，就是将环境的目标分布从$f$改成$p_g$，目标函数变成</p>
<p>$$
J(\pi)=\mathbb{E}_{\textstyle{s_{t+1}\sim \mathcal{T}(\cdot\mid s_t,a_t),\atop a_{t}\sim \pi(\cdot\mid s_{t},g),g\sim f }}\left[\sum_t \gamma^tr(s_t,a_t,g)\right]
$$</p>
<p>$f$可以是一个规则，能够从过去的经验中选择$g$，或者是根据某种准则学习出的一个函数。</p>
<p>下面介绍几种选择子目标的方法。</p>


<h3 class="relative group">1. 中间难度 Intermediate difficulty 
    <div id="1-%E4%B8%AD%E9%97%B4%E9%9A%BE%E5%BA%A6-intermediate-difficulty" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#1-%E4%B8%AD%E9%97%B4%E9%9A%BE%E5%BA%A6-intermediate-difficulty" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p><em>准确评估到达目标的难度有助于确定在智能体的能力范围内学习什么，一个合理的目标应该超出智能体的范围（why？）</em></p>
<ul>
<li><em>Automatic goal generation for reinforcement learning agents.</em> 根据奖励给难度打分，用来把目标标记成正/负采样，然后训练一个GAN模型来生成具有合理难度分数的目标。</li>
<li><em>Learning with amigo: Adversarially motivated intrinsic goals.</em> 提出了一种 teacher-student 框架。teacher充当目标提供者，尝试提供难度递增的目标。teacher从student在提供的目标上的表现中学习，student从环境中得到的外部奖励和teacher中得到的内部奖励学习。</li>
<li><em>Automatic curriculum learning through value disagreement.</em> 把Q函数认知的不确定性看成是实现目标难度的衡量，用于构建一个分布来采样目标。当不确定性过高时，目标应该处于策略的知识边界，以一个合理的难度来学习。</li>
</ul>


<h3 class="relative group">2. 探索意识 Exploration awareness 
    <div id="2-%E6%8E%A2%E7%B4%A2%E6%84%8F%E8%AF%86-exploration-awareness" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#2-%E6%8E%A2%E7%B4%A2%E6%84%8F%E8%AF%86-exploration-awareness" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>由于稀疏奖励限制了策略的学习效率，我们应该提高策略的探索能力，尽可能覆盖没有遇到的目标和状态。
在GCRL，许多研究者们提议<strong>使用子目标采样</strong>来得到一个更好的探索能力。</p>
<ul>
<li><em>Maximum entropy gain exploration for long horizon multi-goal reinforcement learning.</em> 提出采样使过去<strong>已实现目标</strong>的熵最大化的目标，以提高目标空间（goal space）的覆盖范围。（这一点和SAC的目的很像）</li>
<li><em>State-covering self-supervised reinforcement learning.</em> 训练一个自监督的生成式模型来生成行为目标，该目标在过去的经验中对已实现目标进行了近似均匀的倾斜，表现为最大化<strong>期望目标</strong>分布的熵。</li>
<li><em>Unsupervised controlthrough non-parametric discriminative rewards.</em> 选择从不同的replay buffer中均匀地采样过去已实现的目标作为行为目标，这些储存的目标间的距离要尽可能地远。</li>
<li><em>Visual reinforcement learning with imagined goals.</em> 选择从经验回放中均匀地采样过去已实现的目标，用这些历史目标拟合生成式模型来生成行为目标。</li>
<li><em>Exploration via hindsight goal generation</em> 通过 Wasserstein Barycenter 问题，生成了一组事后目标以进行探索。把生成的目标作为隐性课程，可以有效地使用当前值函数来学习。</li>
<li><em>Dynamical distance learning for semi-supervised and unsupervised skill discovery</em> 学习一个距离函数，并从replay buffer中选择距离初始状态最远的目标来提高探索效率。</li>
</ul>


<h3 class="relative group">3. 从经验中搜索 Searching from experience. 
    <div id="3-%E4%BB%8E%E7%BB%8F%E9%AA%8C%E4%B8%AD%E6%90%9C%E7%B4%A2-searching-from-experience" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#3-%E4%BB%8E%E7%BB%8F%E9%AA%8C%E4%B8%AD%E6%90%9C%E7%B4%A2-searching-from-experience" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>历史经验包含导向实现某些目标的路径点，可以使人类或智能体获益。</p>
<ul>
<li><em>Hindsight planner.</em> 从一个采样的回合中选择 $k$ 个关键的中间路径点，并训练一个RNN网络，根据给定的起始和终止点来生成序列。再应用这些中间点作为期望目标的序列来与环境交互。</li>
<li><em>Search on the replay buffer: Bridging planning and reinforcement learning.</em> 在 replay buffer 上建立图，把状态作为节点，从起始状态到目标状态的距离作为边的权重。该工作利用了二值奖励下的状态值可以近似成到目标的距离这一直觉，使用图搜索去寻找路径点序列来实现目标，并不断通过在路径点上学习的策略来采取动作。</li>
</ul>


<h3 class="relative group">4. Model-based planning 和 Learn from experts. 
    <div id="4-model-based-planning-%E5%92%8C-learn-from-experts" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#4-model-based-planning-%E5%92%8C-learn-from-experts" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>model-based RL 与 learning from demonstrations 不在笔者的考虑范围内，故不在此翻译。</p>


<h2 class="relative group">Relabeling in GCRL 
    <div id="relabeling-in-gcrl" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#relabeling-in-gcrl" aria-label="锚点">#</a>
    </span>        
    
</h2>
<p>与子目标选择类似，relabeling也替换了初始的期望目标以提高学习效率。二者的区别是：</p>
<ul>
<li>Relabeling 关注于在训练前替换经验容器中的历史数据；</li>
<li>Sub-goal Selection 关注于改变采样经验的分布。
为了实现重标记操作，理论上需要一个重标记函数 $h$ 去替换从 replay buffer 采样出的 transition 中的目标，并重新计算奖励
$$
(s_t, a_t, g_t, r_t) \gets (s_t, a_t, h(·), r(s_t, a_t, h(·)))
$$</li>
</ul>


<h3 class="relative group">1. 事后回放 Hindsight relabeling. 
    <div id="1-%E4%BA%8B%E5%90%8E%E5%9B%9E%E6%94%BE-hindsight-relabeling" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#1-%E4%BA%8B%E5%90%8E%E5%9B%9E%E6%94%BE-hindsight-relabeling" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>该方法可以追溯到最著名的 Hindsight experience replay(HER)，想法源自于人类从失败的经验中来学习。HER 使用replay buffer相同轨迹中已实现目标来替换期望目标，同时减轻稀疏奖励的问题。</p>
<ul>
<li>
<p><em>Curriculum-guided hindsight experience replay.</em> 提出 CHER，使用课程重标记方法自适应地从失败经验中选择重标记目标。课程的评价标准是与期望目标的相似度和目标的多样性。</p>
</li>
<li>
<p><em>Goal densitybased hindsight experience prioritization for multi-goal
robot manipulation reinforcement learning.</em> 为已实现目标学习了一个稠密模型，用于优先标记少见的目标。以实现在少见经验中探索来提高采样效率。</p>
</li>
</ul>


<h3 class="relative group">2. Relabeling by learning. 
    <div id="2-relabeling-by-learning" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#2-relabeling-by-learning" aria-label="锚点">#</a>
    </span>        
    
</h3>
<ul>
<li><em>Guided goal generation for hindsight multigoal reinforcement learning</em>
从过去的经验选择重标记目标限制了重标记目标的多样性，为了减轻该问题并能够重标记未见过的目标，这篇文章通过隐式地建模策略表现和已实现目标之间的关系来训练一个条件生成式RNN网络，用于根据当前策略的平均返回值生成重标记目标。</li>
</ul>


<h3 class="relative group">3. 先见之明 Foresight. 
    <div id="3-%E5%85%88%E8%A7%81%E4%B9%8B%E6%98%8E-foresight" class="anchor"></div>
    
    <span
        class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100">
        <a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700"
            style="text-decoration-line: none !important;" href="#3-%E5%85%88%E8%A7%81%E4%B9%8B%E6%98%8E-foresight" aria-label="锚点">#</a>
    </span>        
    
</h3>
<p>人类不仅可以从失败中学习，还可以基于当前状态对未来作出规划。</p>
<ul>
<li><em>Mapgo: Modelassisted policy optimization for goal-oriented tasks.</em>  学习一个动态模型用来生成用于重标记的虚拟轨迹。先见目标重标记可以防止标记局限于历史数据的同质目标，规划出当前策略可以实现的新目标。</li>
</ul>

          
          
          
        </div>
        
        

        
        

          
      </div>
     
      
      
        
        
          
          
        
      <script>
        var oid = "views_posts\/9-GCRL.md"
        var oid_likes = "likes_posts\/9-GCRL.md"
      </script>
      
      
      <script type="text/javascript" src="/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js" integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q&#43;oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script>
      
  
    </section>
  <footer class="pt-8 max-w-prose print:hidden">

    
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="flex group mr-3" href="/posts/8-rher%E5%A4%8D%E7%8E%B0/">
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Relayed HER复现结果与分析</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2023-10-24T13:11:22&#43;08:00">24 October 2023</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="flex text-right group ml-3" href="/posts/10-git/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >常用 git 命令</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2023-12-21T13:11:22&#43;08:00">21 December 2023</time>
                  
                </span>
              </span>
              <span
                class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400"
                >&rarr;</span
              >
              <span
                class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400"
                >&larr;</span
              >
            </a>
          
        </span>
      </div>
    </div>
  


    
  </footer>
</article>

      <div id="top-scroller" class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0">
  <a href="#the-top"
    class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
    aria-label="返回顶部" title="返回顶部">
    &uarr;
  </a>
</div>
    </main><footer id="site-footer" class="py-10 print:hidden">
  
  
    
  
  <div class="flex items-center justify-between">

    
    
    <p class="text-sm text-neutral-500 dark:text-neutral-400">
      &copy;
      2024
      
    </p>
    

    
    
    <p class="text-xs text-neutral-500 dark:text-neutral-400">
      
      
      由 <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
        href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
        href="https://blowfish.page/" target="_blank" rel="noopener noreferrer">Blowfish</a> 强力驱动
    </p>
    

  </div>
  <script>
    
    mediumZoom(document.querySelectorAll("img:not(.nozoom)"), {
      margin: 24,
      background: 'rgba(0,0,0,0.5)',
      scrollOffset: 0,
    })
    
  </script>
  
  
  <script type="text/javascript" src="/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js" integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh&#43;sCQ0E53ghYrxgYqw&#43;0GCRyIEpA=="></script>
  
  
</footer>
<div
  id="search-wrapper"
  class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]"
  data-url="http://localhost:1313/"
  style="z-index:500"
>
  <div
    id="search-modal"
    class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"
  >
    <header class="relative z-10 flex items-center justify-between flex-none px-2">
      <form class="flex items-center flex-auto min-w-0">
        <div class="flex items-center justify-center w-8 h-8 text-neutral-400">
          

  <span class="relative block icon">
    <svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>

  </span>


        </div>
        <input
          type="search"
          id="search-query"
          class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent"
          placeholder="搜索"
          tabindex="0"
        />
      </form>
      <button
        id="close-search-button"
        class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400"
        title="关闭 (Esc)"
      >
        

  <span class="relative block icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentColor" d="M310.6 361.4c12.5 12.5 12.5 32.75 0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3L54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75 0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-105.4 105.4L310.6 361.4z"/></svg>

  </span>


      </button>
    </header>
    <section class="flex-auto px-2 overflow-auto">
      <ul id="search-results">
        
      </ul>
    </section>
  </div>
</div>

  </div>
</body>

<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js"
  data-id="jou072" data-description="Support me on Buy me a coffee!" data-message="Buy me a coffee?"
  data-color="#FFDD00" data-position="Right" data-x_margin="18" data-y_margin="18"></script>

</html>
