


[{"content":"","date":"2 April 2025","externalUrl":null,"permalink":"/authors/haoran-zhou/","section":"Authors","summary":"","title":"Haoran Zhou","type":"authors"},{"content":"","date":"2 April 2025","externalUrl":null,"permalink":"/tags/rl/","section":"Tags","summary":"","title":"Rl","type":"tags"},{"content":"Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning —— Yuanpei Chen, Yaodong Yang, et al.\n作者在不同难度的 handover 任务上分别使用 PPO 和 MAPPO 进行了对比实验，结果显示在大部分的任务上，PPO 的表现优于 MAPPO。但是，任务越困难、越需要双手合作时，PPO 和 MAPPO 之间的性能差异越小。当任务非常困难时，MAPPO 的效果超过了PPO，这表明多智能体算法可以提高双手合作操作的性能。\nDynamic Handover: Throw and Catch with Bimanual Hands —— Yuanpei Chen, et al. 通常情况下，Single-Agent 比 Multi-Agent 更容易训练，因为 SA 设置下 thrower 和 catcher 可以互相访问两者的状态。但实验结果显示 MA 的表现更好，原因是 SA 接受了更多的信息, 会更容易在环境中发生过拟合, 导致泛化能力没有 MA 强. MARL 能有效减少 sim2real 的 gap, 因为每个智能体使用了局部观测来改进每个策略的鲁棒性. Learning Dexterous Bimanual Catch Skills through Adversarial-Cooperative Heterogeneous-Agent Reinforcement Learning —— Taewoo Kim, Youngwoo Yoon, and Jaehong Kim 一个智能体抛球，一个智能体（双臂）接球，算法用的是HAPPO,创新点是提出的 aggregate reward\nDexCatch: Learning to Catch Arbitrary Objects with Dexterous Hands —— Fengbo Lan, Shengjie Wang, et al. 纯仿真+基于状态的观测，点云降到2维提取物体形状特征训练多物体泛化，算法是PPO，创新点：引入Lyapunov function辅助值函数估计，增强成功阶段的稳定性。\nthe basic PPO algorithm encounters two key challenges: How to achieve efficient throwing and stable catching without falling? We make some algorithmic modifications to the basic PPO algorithm. First, to accelerate the learning of throwing, we design an intrinsic advan4 tage and add penultimate normalization layers in the network. Secondly, a stable catching behaviour is holding the object in the palm without falling the object. As depicted in Fig. 3, the maximum sum reward (optimality) can not guarantee the generation of the stable catching behaviour. To encourage more stable catching, we include the Lyapunov stability condition in policy learning.\n","date":"2 April 2025","externalUrl":null,"permalink":"/posts/algorithm/10-throw-catch/","section":"Posts","summary":"","title":"基于强化学习的双臂动态抛接任务","type":"posts"},{"content":"","date":"29 January 2025","externalUrl":null,"permalink":"/tags/ml/","section":"Tags","summary":"","title":"Ml","type":"tags"},{"content":" Finetuning # 为了从图像中提取物体的位置信息，对物体进行了分割。使用 resnet18 分别对物体的 mask 和 rgb 图像进行特征提取，联合起来作为观测输入。这样在训练时任务的成功率有明显提高，但在测试时发现成功率接近0. 经过排查，发现测试时 mlp 预测的物体位置与真实值差异很大，并且预测值的分布比较集中。 针对上述问题进行排查，大部分原因集中在 BN 层的处理上：\nBN 层在 model.train 的时候使用当前 batch 与历史的均值和方差的加权平均进行归一化，而 model.eval 时使用全局均值和方差进行归一化。因此导致验证成功率低的原因之一可能是训练时和测试时的输入分布不一致。经过排查发现每次输入的图像都进行了缩放和归一化，因此并不是这个原因。 未能在测试时正确地 model.eval。eval 模式下可以禁用 dropout，并使 BN 层根据全局均值和方差进行归一化。 训练时冻结了 BN 层的预训练参数，导致在 resnet 中的 bn 层所学习到的参数是基于Imagenet数据集的数据分布的并且被冻结后不会再学习，如果在验证时使用了其它分布上的验证集，会是预测精度降低。这个原因和 a 类似。经过排查发现验证集的分布和训练时的分布一致，并不是这个原因。 超参数 # Conv lr=1e-5，att lr=1e-4 mask+rgb背景/噪声背景 resnet14【2，2，1，1】 Conv lr=1e-4，att lr=1e-4 mask+rgb背景/噪声背景 resnet14【2，2，1，1】 Finetune 时卷积层的学习率设置为 1e-5 比较好 Finetune 时 BN 层需要冻结 rm,rv，冻结 weight 和 bias 对训练效果影响不大 Adam 和 AdamW 区别不大 ","date":"29 January 2025","externalUrl":null,"permalink":"/posts/algorithm/8-resnet/","section":"Posts","summary":"","title":"ResNet调参","type":"posts"},{"content":"","date":"21 November 2024","externalUrl":null,"permalink":"/tags/tools/","section":"Tags","summary":"","title":"Tools","type":"tags"},{"content":" Conda 源配置 # channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2 - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/simpleitk auto_activate_base: false show_channel_urls: true 查看当前源\nconda config --show channels conda config --show-sources 删除源\nconda config --remove channels xxx ssh # 想用 A 主机通过 ssh 连接到 B 主机，报错 Connection refused。\n输入下面的命令，查看是否有 sshd 字样输出\nps -e | grep ssh 如果没有说明 sshd 没有启动，运行\nsystemctl restart sshd 如果报错 Failed to restart sshd.service: Unit sshd.service not found.\nsudo apt install openssh-server ","date":"21 November 2024","externalUrl":null,"permalink":"/posts/15-%E5%AE%9E%E7%94%A8%E5%B7%A5%E5%85%B7%E9%85%8D%E7%BD%AE/","section":"Posts","summary":"","title":"实用工具配置","type":"posts"},{"content":" 1.类 # 在了解类之前，我们需要首先了解一下什么是面向对象。\n1.1面向对象的三大特征 # （来源于百度百科）\n封装性：封装使数据和加工该数据的方法（函数）封装为一个整体，以实现独立性很强的模块，使得用户只能见到对象的外特性（对象能接受哪些消息，具有那些处理能力），而对象的内特性（保存内部状态的私有数据和实现加工能力的算法）对用户是隐蔽的。封装的目的在于把对象的设计者和对象者的使用分开，使用者不必知晓行为实现的细节，只须用设计者提供的消息来访问该对象。 继承性：继承性是子类自动共享父类之间数据和方法的机制。它由类的派生功能体现。一个类直接继承其它类的全部描述，同时可修改和扩充。继承具有传递性。继承分为单继承(一个子类只有一父类)和多重继承(一个类有多个父类)。继承概念的实现方式有二类：实现继承与接口继承。实现继承是指直接使用基类的属性和方法而无需额外编码的能力；接口继承是指仅使用属性和方法的名称、但是子类必须提供实现的能力。 多态性：所谓多态就是指对象根据所接收的消息而做出动作，同一消息为不同的对象接受时可产生完全不同的行动，这种现象称为多态性。多态机制使具有不同内部结构的对象可以共享相同的外部接口。 1.2类的访问权限 # 在C++中，我们使用访问说明符加强类的封装性，一个类可以包含0个或多个访问说明符。\n定义在public说明符之后的成员在整个程序内可被访问，public成员定义类的接口。 定义在private说明符之后的成员可以被该类的中的函数以及友元函数访问，但是不能被使用该类的代码访问，private部分封装了类的实现细节。 定义在protected说明符之后的成员可以被该类中的函数、子类的函数、以及其友元函数访问,但不能被该类的对象访问。 2.vector # 标准库类型vector（容器）表示对象的集合，其中需要所有的对象类型都相同。集合中的每个对象都有一个与之对应的索引，用于访问对象。 vector是一个类模板，编译器根据模板创建类或函数的过程为实例化，使用模板时，需要指出编译器应把类或函数实例化成何种类型，即在模板名字后跟一对尖括号，在括号中放上信息。\n则vector\u0026lt;int\u0026gt; preorderTraversal表示，在pre中保存int类型的对象。\n2.1 定义和初始化vector对象 # 方法 含义 vector\u0026lt;T\u0026gt; v1 v1是一个空容器，类型为T vector\u0026lt;T\u0026gt; v2(v1) v2中包含有v1所有元素的副本 vector\u0026lt;T\u0026gt; v2 = v1 等价于v2(v1) vector\u0026lt;T\u0026gt; v3(n, val) v3包含了n个val vector\u0026lt;T\u0026gt; v4(n) v4包含了n个初始化T的对象 vector\u0026lt;T\u0026gt; v5{a, b, c\u0026hellip;} v5包含了初始个数的元素，每个元素被赋予相应的值 vector\u0026lt;T\u0026gt; v5 = {a, b, c\u0026hellip;} 等价于v5{a, b, c\u0026hellip;} 2.2 向vector对象中添加元素 # vector的成员函数push_back能够把一个值当成vector对象的尾元素push到vector对象的尾端。 例如ret.push_back(root-\u0026gt;val); 注：如果在循环体内部包含有向vector对象添加元素的语句，不能使用范围for循环。\n2.3 其它vector操作 # 操作 含义 v.empty() 如果v中不含有任何元素，返回真，否则返回假 v.size() 返回v中元素的个数 v.push_back(t) 向v的尾端添加一个值为t的元素 v[n] 返回v中第n个位置上元素的引用 v1 = v2 用v2中元素的拷贝替换v1中的元素 v1 = {a, b, c\u0026hellip;} 用列表中元素的拷贝替换v1中的元素 v1 == v2 判断v1和v2中的元素数量与对应元素值都相等 v1 != v2 \u0026lt;, \u0026lt;=, \u0026gt;, \u0026gt;= 以字典顺序进行比较 注：不能用下标形式添加元素。\n2.4 迭代器 # 所有标准库容器都可以使用迭代器，它的对象是容器中的元素或string对象中的字符，有迭代器的类型同时拥有返回迭代器的成员，使用迭代器可以访问某个元素或对其进行操作，迭代器和指针类似，有有效与无效之分。 其中， begin成员返回指向容器第一个元素的迭代器； end成员返回指向容器尾元素的下一位置的迭代器； 例如：\nauto b = v.begin(), e = v.end(); b表示v的首元素，e表示v尾元素的下一位置，它们的类型与begin和end的返回值类型相同\n标准容器迭代器的运算符：\n运算符 含义 *iter 返回迭代器所指元素的引用 iter-\u0026gt;mem 解引用iter并获取该元素的名为mem的成员，等价于(*iter).mem ++iter 令iter指向容器中的下一个元素 \u0026ndash;iter 令iter指向容器中的上一个元素 iter1 == iter2 判断两个迭代器是否相等，如果两个迭代器指向同一元素或是同一个容器的尾后迭代器则相等 iter1 != iter2 注：凡是使用了迭代器的循环体，都不要向迭代器所属的容器添加元素。\nvector和string提供了更多的运算符：\n运算符 含义 iter + n 迭代器指向的位置向前移动n个位置 iter - n 迭代器指向的位置向后移动n个位置 iter += n 将iter加n的结果赋给iter iter -= n 将iter减n的结果赋给iter iter1 - iter2 计算两个迭代器之间的距离 \u0026gt;、\u0026gt;=、\u0026lt;、\u0026lt;= 关系运算， 比较两迭代器指向元素位置的前后关系 1.顺序容器 # 1.1 顺序容器类型 # 类型详情见最上方思维导图，在此不过多赘述。\nstring和vector将元素保存在连续的内存空间中，能够使用下标进行快速的随机访问，但是在中间添加或删除元素非常耗时。 list和forward_list与上述互补，即令任何位置的添加和删除操作变得快速，但是不支持元素的随机访问。且forward_list没有size操作。 deque与vector类似，但是它在两端的添加或删除操作很快。 array的大小是固定的，不支持添加和删除操作。 1.2 容器操作 # 常见操作详见DAY1，下面进行一些补充\n成员 含义 c.back() 返回c中尾元素的引用 c.front() 返回c中首元素的引用 a.swap(b) 等价于 swap(a, b) 交换a和b的元素 c.emplace(inits) 使用inits构造c中的一个元素 c.erase(args) 删除args指定的元素 c.clear() 删除c中的所有元素，返回void c.pop_back() 删除c中尾元素 c++11 c.pop_front() 删除c中首元素 c++11 c.erase(p) 删除迭代器p所指定的元素，返回一个指向被删元素之后元素的迭代器 c.erase(b, e) 删除迭代器b和e范围内的元素，返回一个指向最后一个被删元素之后元素的迭代器 插入操作 含义 c.push_front(t) 在c的头部创建一个值为t或由args创建的元素 c.emplace_front(args) c.insert(args) 将args中的元素拷贝进c c.insert(p, t) 在迭代器p指向的元素之前创建一个值为t或由args创建的元素，返回指向新添加元素的迭代器 c.emplace(args) c.insert(p, n, t) 在迭代器p指向的元素之前插入n个值为t的元素，返回指向新添加的第一个元素的迭代器，若范围为空，则返回p c.insert(p, b, e) 将迭代器b和e指定的范围内的元素插入到迭代器p指向的元素之前，b和e不能指向c中的元素，返回指向新添加的第一个元素的迭代器，若范围为空，则返回p c.insert(p, il) il是一个花括号包围的元素值列表，将这些定值插入到迭代器p指向的元素之前，返回指向新添加的第一个元素的迭代器，若范围为空，则返回p 注： 所有容器都支持相等或不等运算符； vector和string不支持push_front和emplace_front forward_list不支持push_back和emplace_back array不支持这些操作 向一个vector、string和deque插入元素会使所有指向容器的迭代器、引用和指针失效。 迭代器：详见DAY2,其中，forward_list迭代器不支持递减运算符，并且DAY1中所列举的算术运算只能应用于string、vector、deque和array的迭代器。 迭代器范围：由一对迭代器表示，通常称为begin和end，迭代器范围中的元素包含begin所表示的元素以及begin和end之间所有的元素（end指向尾元素之后的位置），这种元素范围称为左闭合区间。 因此可以用如下的代码用循环来处理一个元素范围：\nwhile(begin != end){ *begin = val; ++begin; } 关于list和forward_list的操作，可以看这篇博客https://blog.csdn.net/u013006553/article/details/78158717\n1.3 容量 # 操作 含义 c.resize(n) 调整c的大小为n个元素 c.resize(n, t) 调整c的大小为n个元素， 任何新添加的元素都初始化为值t c.capacity() 不重新分配内存空间的话，c可以保存多少元素 c.reserve(n) 分配至少能容纳n个元素的内存空间 2.容器适配器 # 除了顺序容器，标准库还定义了三个顺序容器适配器：栈适配器stack、队列适配器queue和priority_queue。\n栈适配器新操作 含义 s.pop() 删除栈顶元素 s.push(item) 压栈 s.top() 返回栈顶元素 队列适配器新操作 含义 q.pop() 返回queue的首元素或priority_queue的最高优先级的元素 q.front() 返回首元素或尾元素 q.back() 只适用于queue q.top() 返回最高优先级元素，只适用于priority_queue q.push(item) 在queue末尾或priority_queue中恰当的位置创建一个元素 3.关联容器 # 联容器中的元素是根据关键字储存的，支持普通容器操作，但是不支持顺序容器位置相关的操作。 具体内容将在DAY4中更新。\n4.intersect()求交集 # c=intersect(a,b)找出向量a与b的相同元素，并按升序返回到c中。\n3. 其他 # 3.1 nullptr # 在c语言中，NULL实际上是一个空指针，在使用中被隐式转换成其它类型； 而c++是强类型语言，不能进行隐式类型转换，NULL直接定义为0，定义nullptr为空指针。\n3.2 输入输出 # 在初入C++时，会发现和C的头文件引用有了区别，我们用了#include\u0026lt;iostream\u0026gt; iostream库包含两个基础类型istream和ostream，分别表示输入流和输出流，流的意思是字符是顺序生成或消耗的。 标准库定义了四个IO对象：\n输入，使用名为cin的istream类型的对象； 输出，使用名为cout的ostream类型的对象； 输出警告和错误信息，使用名为cerr的ostream类型的对象； 输出程序运行时的一般性信息，使用名为clog的ostream类型的对象。 1.结合输出运算符\u0026laquo;，可以完成以下操作：\nstd::cout \u0026lt;\u0026lt; \u0026#34;hello world\u0026#34; \u0026lt;\u0026lt; std::endl; 第一个\u0026laquo;运算符将右侧给定的值(\u0026ldquo;hello world\u0026rdquo;)写到左侧给定的ostream对象(cout)中，下一个\u0026laquo;运算符又将endl写到左侧的新对象中。其中endl是一个被称为操纵符的特殊值，写入它的效果是结束当前行并将缓冲区中的内容刷到设备中。\n2.结合输入运算符\u0026raquo;，可以完成以下操作：\nint v1 =0, v2 = 0; std::cin \u0026gt;\u0026gt; v1 \u0026gt;\u0026gt; v2; 同理，\u0026raquo;从左侧给定的istream(cin)读入数据，并存给右侧的给定对象(v1)，返回左侧的对象作为计算结果，第二个\u0026raquo;又重复了一遍上述操作，即从cin中读入两个值，分别存入v1和v2中。\n3.3 命名空间namespace # 我们可以发现程序中使用std::cout而不是cout，因为前缀std::指出名字cout是定义在名为std的命名空间中的。命名空间可以帮助我们避免名字定义冲突，以及使用库中相同名字导致的冲突，标准库定义的所有名字都在std中。 其中::是作用域运算符，它指出我们想使用定义在命名空间std中的名字cout。\n每次都使用std::cout太过于麻烦，我们可以对命名空间进行using声明，声明后就无需前缀也能使用所需的名字了。 using声明的格式：using namespace::name; 例如，\n#include\u0026lt;iostream\u0026gt; using std::cin; using std::cout; using std::endl; 于是，我们就可以愉快地直接使用cin, cout, endl了。\n注：头文件中不应包含using声明。\n","date":"26 February 2024","externalUrl":null,"permalink":"/posts/12-c++%E5%AF%B9%E8%B1%A1/","section":"Posts","summary":"","title":"C++ 对象","type":"posts"},{"content":"","date":"26 February 2024","externalUrl":null,"permalink":"/tags/moe/","section":"Tags","summary":"","title":"Moe","type":"tags"},{"content":" Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. # Mixture-Of-Experts # 现在的模型越来越大，训练样本越来越多，每个样本都需要经过模型的全部计算，这就导致了训练成本的平方级增长。\n为了解决这个问题，该文章提出了 Mixture-Of-Experts (Moe，稀疏门控制的专家混合层)，将大模型拆分成多个小模型。对于一个样本来说，无需经过所有的小模型去计算，而只是激活一部分小模型进行计算，这样就节省了计算资源。\n那么如何决定一个样本去经过哪些小模型呢？这就引入了一个稀疏门机制，即样本输入给这个门，得到要激活的小模型索引，这个门需要确保稀疏性，从而保证计算能力的优化。\n模型结构 # MoE是一个层，而不是一整个模型。这个模型结构包含一个门网络来决定激活哪个expert，同时包含n个expert网络，这n个expert网络一般是同结构的。\n这个门网络其实比较简单，就是一个dense layer再来一个softmax。 $$ G_\\sigma(x)=\\text{Softmax}(x\\cdot W_g) $$ MoE层的输出如下，当$G(x)_i=0$的时候，对应的 expert 就不会激活。 $$ y=\\sum_{i=1}^{n}G(x)_iE_i(x) $$\nHybrid hierarchical learning for solving complex sequential tasks using the robotic manipulation network ROMAN # Overview # 该文章提出了一种混合分层学习 （hybrid hierarchical learning，HHL）框架：机器人操作网络（ROMAN），用于解决复杂的机械臂长期操作任务问题。ROMAN 结合了 behavioural cloning, imitation learning and reinforcement learning。它由一个中央操作网络（manipulation network，MN）组成，用于结合和编排包含各种神经网络的集合，每个网络用于为不同的可重新组合的子任务生成正确的连续动作 （High-level task decomposition）。\n当涉及到与稀疏奖励相关的复杂任务时，Hierarchical Learning（HL） 提供了多种好处，因为它允许将任务分解为更容易接近的问题，即子任务。\n与 RL 结合：HRL 仍然从根本上依赖于强化学习，因此受到稀疏奖励、复杂规划任务和难以使用先验知识的不利影响。 与 IL 结合：当这些 HL 策略应用到 IL(HIL)时，以 teacher–student 的方式能够更容易区分专业专家和获得专业人类技能。 MoEs Architecture # 作者认为，基于 mixture of experts (MoE) 的分层方法能够解决复杂的长期操作任务。\nROMAN 的分层结构如图所示，展示了专家网络是如何协调结合和激活的。每个专家网络是一个多层感知机，专门负责一类基础的操作任务。\n作者将 MoE 层中的门网络称为 manipulation network，MN，其训练用于高级的场景理解和专家的编排，这种方法能够提高对未演示情境的泛化性。此外，该算法通过激活多个专家权值来克服局部最小值，从而增强了算法在求解长期序列任务时的鲁棒性。\nMN 能够为专家网络分配权重 (∈(0, 1))，最终的输出为这些专家输出动作的加权和： $$ \\sum_{i=1}^m\\sum_{j=1}^n \\alpha_iw_j $$ 每个专家全部数量 (m = 4)的动作 (αi) 由一组与该层次中专家总数(n = 7)相对应的权重 (wj) 控制。我们要确保所有权重的总和不超过1，使用 softmax 函数 σ 对由 MN 分配的权重总和归一化。 $$ σ(z)_i =\\frac{e^{z_i}}{\\sum^K_{j=1} e^{z_i}} \\medspace for\\medspace i = 1,…, K\\medspace and\\medspace z =(z_1,…, z_K)∈ℝ^K $$ 输入 z 表示权重向量，每个元素表示每个专家的权重，K = 7，代表所有7个不同的专家。\nTraining Procedure # ROMAN 的训练过程由两个阶段组成：\n1. 使用 BC 热启动策略。 # 因为 BC 受限制于所看到的演示，无法泛化到超出分布范围的状态，导致当智能体鼓励不在所提供演示内的新轨迹时出现误差。因此，最好不独立地使用 BC，以允许智能体探索更多的样本，改进已演示的行为，同时保持较小的演示数据集。同时在计算与 BC loss 相对应的单独 RL 梯度时添加奖励项。\n作者在工作中实现 BC 的方法是，使用由演示提供的 state-action 数据集，通过监督学习去最小化期望动作和实际动作间的均方误差来训练一个 NN 策略。\n2. 通过 PPO 算法更新策略，分别使用来自环境（RL）和 判别器（discriminator） network (GAIL) 的 $r_E$ 和 $r_I$。 # 为了有效地匹配一段 horizon 内的演示数据，作者在 BC 的 cutoff point 后又使用了一种逆强化学习方法——GAIL 进行后面的训练，以最小化智能体策略和 demonstrator 间的差异。然而 GAIL 没有直接用于更新策略的参数，作者使用了从 GAIL 中获得的代理模仿奖励信号。\n接下来介绍具体的实现方法。首先从演示数据集中采样专家轨迹 $\\tau_E$，从生成器 $G$ 中采样智能体轨迹 $\\tau_A$。然后由判别器 $D$ 进行打分来给出奖励。$D$ 也是由一个独立的 NN 实现的，如果专家轨迹和智能体轨迹的差异减小，则奖励 $G$ 。 判别器会被训练得越来越严格，使智能体能够更好地模仿人类专家所演示的行为。可以表示成下面的公式： $$ E_{τ_E} [∇ log(D(s_t, a_t))] + E_{τ_A} [∇ log(1 − D(s_t, a_t))] $$\n$E_{τ_E}$ 和 $E_{τ_A}$ 作为 $D$ 网络的输入，输出一个 0 和 1 之间的连续值，越接近 1 意味着智能体或生成器的轨迹越接近专家。本质上是最小化差异、最大化模仿。因此，$D$ 可以用作于一个内部奖励信号来训练 $G$ 去模仿专家的演示数据。\n此外，为了使智能体进一步探索与所演示的操作相比可以提高性能的其他操作。作者仅令判别器使用演示轨迹中的 state($s_t$)，这会导致更多的探索，并将鼓励在与强化学习相结合的情况下做出在演示序列之外的行为。上式可以改写为 $$ E_{τ_E} [∇ log(D(s_t))] + E_{τ_A} [∇ log(1 − D(s_t))] $$ 这可能会使智能体无法进一步探索其他动作，但实际上可能导致基于状态空间的更好的适应，避免对相同模仿的简单复制。\n此外，作者还引入了一个小的任务相关的外部奖励信号，以避免最终策略仅依赖模仿，有能力适应新的示例。作者使用内部(来自IL)和外部任务相关奖励来更新策略，其中IL奖励按最高权重缩放，并按程度作为主要学习信号提供者。这允许 ROMAN 在最复杂的专家序列激活期间从局部最小值恢复，即使序列没有精确激活或个别专家出现错误。PPO 的策略网络权重在第 k 步更新的公式为 $$ θ_{k+1} = \\underset{θ}{arg max}\\mathbb{E}_{s,a\\sim {\\pi_θk}} [L(s, a, θ_k, θ)] $$\nIntegration of BC, GAIL and RL. # 为了学习使用有限的演示数据来解决长且复杂的顺序任务，作者通过使用奖励项 rI 和 rE 集成了上述算法，以在探索和利用之间取得有效的平衡。\n内部奖励项定义为 $r_I=−log(1−D(s_t)), D(s_t) \\in (0, 1)$，作为一个代理奖励项被PPO用来最大化GAIL目标。当训练GAIL和RL时，奖励可以组合为 $ r = r_Iw_I + r_Ew_E$，$w_I$ 和 $w_E$ 是固定的奖励缩放参数，由于作者提出的 HHL 更多关注于 IL，因此 $w_I \u0026gt; w_E$。奖励 r 用于 PPO 策略更新。\n每个专家的NN结构和门控网络MN如图所示。从环境中提供给每个 NN 的观测信息是由每个专家的目标以及该信息与成功完成给定子任务的相关性决定的。相比之下，MN观察的是整个环境。\nDemonstration acquisition and settings # 关于仿真中物体信息的获取：作者没有直接从仿真中读取物体信息，而是在仿真中加入 RGB 相机来预测感兴趣物体的位姿（OIs）。\n第一类演示被提供给专家网络（N = 20），包括机械臂末端的速度和夹爪的二进制状态（开或关）。专家网络共享相同的动作，专注于不同的操作技能。 第二类演示被提供给 MN（N = 42，7 个场景，每个场景 6 个演示），给定预训练的专家网络，演示包括专家的权重分配。因此，专家演示是针对每个专家的专业技能和目标的，这使得这些预训练的网络可以通过MN演示的顺序来协调，从而连续激活任务。 参考文献 # Shazeer, Noam, et al. \u0026ldquo;Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\u0026rdquo; arXiv preprint arXiv:1701.06538 (2017). 知乎-MoE: 稀疏门控制的专家混合层 Triantafyllidis, Eleftherios, et al. \u0026ldquo;Hybrid hierarchical learning for solving complex sequential tasks using the robotic manipulation network ROMAN.\u0026rdquo; Nature Machine Intelligence 5.9 (2023): 991-1005. ","date":"21 February 2024","externalUrl":null,"permalink":"/posts/algorithm/6-moe/","section":"Posts","summary":"","title":"Mixture-of-Experts in RL for Robotics","type":"posts"},{"content":"","date":"21 December 2023","externalUrl":null,"permalink":"/tags/git/","section":"Tags","summary":"","title":"Git","type":"tags"},{"content":" 配置 github 的 ssh 公钥 # 在新的主机上使用 github 时为了避免输入用户名和密码，可以在 github 账号下配置 ssh 的公钥。 具体步骤如下：\n第一步：检查本地主机是否已经存在ssh key cd ~/.ssh ls //看是否存在 id_rsa 和 id_rsa.pub文件，如果存在，说明已经有SSH Key 如果存在，直接跳到第三步 2. 第二步：生成ssh key 如果不存在ssh key，使用如下命令生成\nssh-keygen -t rsa -C \u0026#34;xxx@xxx.com\u0026#34; //执行后一直回车即可 第三步：获取ssh key公钥内容（id_rsa.pub） cd ~/.ssh cat id_rsa.pub 第四步：Github账号上添加公钥 GITHUB -\u0026gt; SETTINGS -\u0026gt; SSH AND GPG KEYS -\u0026gt; NEW SSH KEY 将公钥内容复制粘贴进去即可。 第五步：验证是否设置成功 ssh -T git@github.com git push代码到远程新分支 # 获取远程代码修改后,想要push到远端与原来不同的新分支，可以使用下面的命令实现：\ngit push origin 本地分支:远端希望创建的分支 例如git下来的分支为master\ngit branch \u0026gt;\u0026gt;\u0026gt; *master git push origin master:my_remote_new_branch #远端即可创建新的分支my_remote_new_branch,提交本地修改 关于配置 # 使更改的.gitignore文件生效 # 运行以下命令，清除 Git 缓存中的所有文件，以确保 Git 会重新读取 .gitignore 文件\ngit rm -r --cached . 这个命令会将 Git 缓存中的所有文件标记为需要删除。\n运行以下命令，将未被 Git 跟踪的文件添加回到 Git 仓库中：\ngit add . 运行以下命令，提交更改\ngit commit -m \u0026#34;Update .gitignore\u0026#34; 这个命令将提交你的更改并将它们添加到 Git 历史记录中。\nsubmodule # 常见场景：当项目依赖并跟踪一个开源的第三方库时，将第三方库设置为submodule\n参考文章 Git中submodule的使用 创建 submodule # 获取 submodule # 如果希望子模块代码也获取到，一种方式是在克隆主项目的时候带上参数 --recurse-submodules，这样会递归地将项目中所有子模块的代码拉取。\n对于已经部署的项目, 可以在当前主项目中执行：\ngit submodule init git submodule update 则会根据主项目的配置信息，拉取更新子模块中的代码。\nCommit # 撤销最近的一次 commit # 仅撤回commit操作，写的代码仍然保留。\ngit reset --soft HEAD^ 参数：\n\u0026ndash;mixed 意思是：不删除工作空间改动代码，撤销commit，并且撤销git add . 操作 这个为默认参数,git reset \u0026ndash;mixed HEAD^ 和 git reset HEAD^ 效果是一样的。\n\u0026ndash;soft\n不删除工作空间改动代码，撤销commit，不撤销git add .\n\u0026ndash;hard 删除工作空间改动代码，撤销commit，撤销git add .\nHEAD^的意思是上一个版本，也可以写成HEAD1 如果你进行了2次commit，想都撤回，可以使用HEAD2\nStash # git stash # 有时，当你在项目的一部分上已经工作一段时间后，所有东西都进入了混乱的状态， 而这时你想要切换到另一个分支做一点别的事情。 问题是，你不想仅仅因为过会儿回到这一点而为做了一半的工作创建一次提交。 可以使用 git stash 或 git stash push 将新的贮藏推送到栈上。\ngit add . git stash Saved working directory and index state WIP on dev-m92u-mj3: 5a62067 添加兼容UDP协议. 如需打开UDP,在CMakeLists.txt中设置USE_SHM=OFF 此时，可以切换分支并在其他地方工作；你的修改被存储在栈上。 要查看贮藏的东西，可以使用 git stash list：\ngit stash list stash@{0}: WIP on dev-m92u-mj3: 5a62067 添加兼容UDP协议. 如需打开UDP,在CMakeLists.txt中设置USE_SHM=OFF 可以通过原来 stash 命令的帮助提示中的命令将你刚刚贮藏的工作重新应用：git stash apply。 如果想要应用其中一个更旧的贮藏，可以通过名字指定它，像这样：git stash apply stash@{2}。 如果不指定一个贮藏，Git 认为指定的是最近的贮藏\nMerge # 修改代码后，将其他人的修改合并到当前分支上 # 问题描述： 在分支A对代码进行修改时，其它人也在分支A上进行了修改并完成推送。现在要把该分支上的最新内容合并到当前的修改。\nRemote # 一个项目push到多个远程Git仓库 # 如下命令查看远程仓库的情况，可以看到只有一个叫 github 的远程仓库。 git remote origin git remote -v origin git@github.com:NoneJou072/IsaacLab.git (fetch) origin git@github.com:NoneJou072/IsaacLab.git (push) 使用如下命令再添加一个远程仓库 git remote add gitlab git@git.n.xiaomi.com:zhouhaoran1/bimanual-hand-gym.git 再次查看远程仓库的情况，可以看到已经有两个远程仓库了。然后再使用相应的命令 push 到对应的仓库就行了。这种方法的缺点是每次要 push 两次。 git remote github oschina git remote -v github https://github.com/zxbetter/test.git (fetch) github https://github.com/zxbetter/test.git (push) oschina https://git.oschina.net/zxbetter/test.git (fetch) oschina https://git.oschina.net/zxbetter/test.git (push) Branch # 新建本地分支 # 新建并切换到新建的分支 git checkout -b iss53 它是下面两条命令的简写： 新建分支 git branch iss53 切换到新建的分支 git checkout iss53 实战：将A仓库的分支合并到B仓库的分支上 # 下载需要进行合并的仓库 B git clone \u0026lt;B\u0026gt; 添加需要被合并的远程仓库 A git remote add gitlab \u0026lt;A\u0026gt; 将 gitlab 作为远程仓库，添加到 本地仓库(origin)中，设置别名为 gitlab(自定义，为了方便与本地仓库origin作区分)\n此时使用 git remote 查看所有远程仓库将看到两个 一个本地默认仓库origin 另外一个我们新增的 gitlab\nmi@mi-LEGION-REN9000K-34IRZ: ~/work/hemera_mujoco$ git remote gitlab origin ··· 3. 把 gitlab 远程仓库（A）中数据抓取到本仓库（B） ```bash git fetch gitlab \u0026lt;分支名称\u0026gt; 第2步 git remote add xxx 我们仅仅是新增了远程仓库的引用，这一步真正将远程仓库的数据抓取到本地，准备后续的更新。\nmi@mi-LEGION-REN9000K-34IRZ: ~/work/hemera_mujoco$ git fetch gitlab dev_carlogo_lab_zhr remote: Enumerating objects: 116, done. remote: Counting objects: 100% (116/116), done. remote: Compressing objects: 100% (93/93), done. remote: Total 116 (delta 33), reused 101 (delta 22), pack-reused 0 Receiving objects: 100% (116/116), 23.66 MiB | 48.74 MiB/s, done. Resolving deltas: 100% (33/33), completed with 2 local objects. From git.n.xiaomi.com:mi_manipulator/hemera_manipulator/hemera_mujoco * branch dev_carlogo_lab_zhr -\u0026gt; FETCH_HEAD * [new branch] dev_carlogo_lab_zhr -\u0026gt; gitlab/dev_carlogo_lab_zhr branch dev_carlogo_lab_zhr -\u0026gt; FETCH_HEAD: 将远程分支 dev_carlogo_lab_zhr 更新到本地的 FETCH_HEAD。\n[new branch] dev_carlogo_lab_zhr -\u0026gt; gitlab/dev_carlogo_lab_zhr: 新分支 dev_carlogo_lab_zhr 被创建并关联到本地的 gitlab/dev_carlogo_lab_zhr 分支。\n基于 gitlab 仓库的 dev_carlogo_lab_zhr 分支，新建一个分支，并切换到该分支，命名为 \u0026ldquo;gitlab_\u0026rdquo; git checkout -b gitlab_ gitlab/dev_carlogo_lab_zhr 此时我们的仓库B就有了一个基于仓库A内容的分支 \u0026ldquo;githubB\u0026rdquo;，后续我们将 \u0026ldquo;githubB\u0026rdquo; 分支代码合并到master就可以了。\n使用 git branch 查看所有分支\nmi@mi-LEGION-REN9000K-34IRZ: ~/work/hemera_mujoco$ git branch * gitlab_ master mobile_arm 我们切换到需要合并的分支 master git checkout mobile_arm 第 4 步我们创建了即将被合并分支 \u0026ldquo;githubB\u0026rdquo; ，默认是在当前分支上的，所以我们需要切换回我们的目标分支。\n合并 git merge gitlab_ --allow-unrelated-histories 然后可以在 vscode 中进行代码合并 合并完后 commit\n","date":"21 December 2023","externalUrl":null,"permalink":"/posts/10-git/","section":"Posts","summary":"","title":"Git食用手册","type":"posts"},{"content":" 回溯算法（Backtracking）：一种能避免不必要搜索的穷举式的搜索算法。采用试错的思想，在搜索尝试过程中寻找问题的解，当探索到某一步时，发现原先的选择并不满足求解条件，或者还需要满足更多求解条件时，就退回一步（回溯）重新选择，这种走不通就退回再走的技术称为「回溯法」，而满足回溯条件的某个状态的点称为「回溯点」。\n回溯算法的基本思想：以深度优先搜索的方式，根据产生子节点的条件约束，搜索问题的解。当发现当前节点已不满足求解条件时，就「回溯」返回，尝试其他的路径。\n具体步骤如下：\n明确所有选择：画出搜索过程的决策树，根据决策树来确定搜索路径。 明确终止条件：推敲出递归的终止条件，以及递归终止时的要执行的处理方法。 将决策树和终止条件翻译成代码： 定义回溯函数（明确函数意义、传入参数、返回结果等）。 书写回溯函数主体（给出约束条件、选择元素、递归搜索、撤销选择部分）。 明确递归终止条件（给出递归终止条件，以及递归终止时的处理方法）。 回溯算法的通用模板：\nres = [] # 存放所欲符合条件结果的集合 path = [] # 存放当前符合条件的结果 def backtracking(nums): # nums 为选择元素列表 if 遇到边界条件: # 说明找到了一组符合条件的结果 res.append(path[:]) # 将当前符合条件的结果放入集合中 return for i in range(len(nums)): # 枚举可选元素列表 path.append(nums[i]) # 选择元素 backtracking(nums) # 递归搜索 path.pop() # 撤销选择 backtracking(nums) 参考：04.02.01 递归算法（第 03 ~ 04 天）\n509. 斐波那契数 # 思路：\n题目直接给出了递归函数 $F(n) = F(n - 1) + F(n - 2)$,\n终止条件为 n=0 或 n=1 ，可以容易地写出递归代码：\nclass Solution { public: int fib(int n) { if(n==0){ return 0; } else if(n==1){ return 1; } return fib(n-1) + fib(n-2); } }; 执行用时分布12ms 消耗内存分布6.23MB\n时间复杂度：$O((\\frac{1 + \\sqrt{5}}{2})^n)$。 空间复杂度：$O(n)$。\n","date":"18 December 2023","externalUrl":null,"permalink":"/posts/leetcode/datawhale%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%9F%BA%E7%A1%80%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E5%9B%9E%E6%BA%AF/","section":"Posts","summary":"","title":"Datawhale数据结构基础课程笔记-(Day7-9)回溯","type":"posts"},{"content":" 递归（Recursion）：指的是一种通过重复将原问题分解为同类的子问题而解决的方法。在绝大数编程语言中，可以通过在函数中再次调用函数自身的方式来实现递归。\n递归的基本思想： 把规模大的问题不断分解为子问题来解决。\n递归的计算可以分成下面两个过程：\n递推过程：先逐层向下调用自身，直到达到结束条件。即将原问题一层一层地分解为与原问题形式相同、规模更小的子问题，直到达到结束条件时停止，此时返回最底层子问题的解； 回归过程：向上逐层返回结果，直到返回原问题的解。即从最底层子问题的解开始，逆向逐一回归，最终达到递推开始时的原问题，返回原问题的解。 参考：04.02.01 递归算法（第 03 ~ 04 天）\n509. 斐波那契数 # 思路：\n题目直接给出了递归函数 $F(n) = F(n - 1) + F(n - 2)$,\n终止条件为 n=0 或 n=1 ，可以容易地写出递归代码：\nclass Solution { public: int fib(int n) { if(n==0){ return 0; } else if(n==1){ return 1; } return fib(n-1) + fib(n-2); } }; 执行用时分布12ms 消耗内存分布6.23MB\n时间复杂度：$O((\\frac{1 + \\sqrt{5}}{2})^n)$。 空间复杂度：$O(n)$。\n104. 二叉树的最大深度 # 思路：\n我们根据题意建立递归函数： 当前根节点的最大深度 = 叶子节点中的最大深度 + 1，函数表示为 $$ maxDepth(root) = max(maxDepth(root\\to left), maxDepth(root\\to right)) + 1 $$ 递归的终止条件是当前节点为空（返回0）或者没有叶子节点（返回1）。\nclass Solution { public: int maxDepth(TreeNode* root) { if(root == nullptr){ return 0; } if(root-\u0026gt;left == nullptr \u0026amp;\u0026amp; root-\u0026gt;right == nullptr){ return 1; } if(root-\u0026gt;left == nullptr){ return maxDepth(root-\u0026gt;right) + 1; } else if(root-\u0026gt;right == nullptr){ return maxDepth(root-\u0026gt;left) + 1; } return max(maxDepth(root-\u0026gt;left), maxDepth(root-\u0026gt;right)) + 1; } }; 执行用时分布12ms 消耗内存分布18.62MB\n时间复杂度：$O(n)$，其中是二叉树的节点数目。 空间复杂度：$O(n)$。\n70. 爬楼梯 # 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。每次可以爬 1 或 2 个台阶。有多少种不同的方法可以爬到楼顶呢？\n思路：\n由于第 n 阶楼梯可以是一次爬1个或2个台阶上来的，因此爬 n 阶楼梯的方法数等于爬 n-1 阶楼梯的方法数加上爬 n-2 阶楼梯的方法数，函数表示为 $$ climbStairs(n) = climbStairs(n-1) + climbStairs(n-2) $$ 终止条件是 n=1， 此时只有一种方法； 或者 n = 2，此时有两种方法。 class Solution { public: int climbStairs(int n) { if(n==1){ return 1; } else if(n==2){ return 2; } return climbStairs(n-1) + climbStairs(n-2); } }; 然后递归的方法会在 n = 44 时超时。 使用动态规划的方法。 class Solution { public: int climbStairs(int n) { vector\u0026lt;int\u0026gt; table; table.push_back(0); table.push_back(1); table.push_back(2); for(int i = 3; i \u0026lt;= n; ++i){ table.push_back(table[i-1] + table[i-2]); } return table[n]; } }; 执行用时分布0ms 消耗内存分布6.27MB 226. 翻转二叉树 # 思路：\n可以把翻转二叉树这个问题分解成逐层翻转各结点子树的子问题。我们交换当前节点的左右节点，然后递归这个这节点的左节点，再递归右节点。\n终止条件是：当前节点是空节点 class Solution { public: TreeNode* invertTree(TreeNode* root) { if(root==nullptr){ return nullptr; } TreeNode* tmp = root-\u0026gt;left; root-\u0026gt;left = root-\u0026gt;right; root-\u0026gt;right = tmp; invertTree(root-\u0026gt;left); invertTree(root-\u0026gt;right); return root; } }; 执行用时分布0ms 消耗内存分布9.72MB 206. 反转链表 # 思路：\n先递推到最后一个节点，令最后的节点指向上一个节点，然后返回上一个节点，作回归。 终止条件：当前节点或下一个节点为空 class Solution { public: ListNode* reverseList(ListNode* head) { // 这里 head 是当前链表的头 if(head == nullptr || head-\u0026gt;next==nullptr){ return head; } //递归传入下一个节点，目的是为了到达最后一个节点 // 这里返回的是已经翻转后的子链表的表头 ListNode* new_head = reverseList(head-\u0026gt;next); // 出栈 // 我们要让下一个结点指向当前结点，即下下个结点是当前结点 head-\u0026gt;next-\u0026gt;next = head; // 由于当前结点指向着下一个结点，我们要断开这个链接 head-\u0026gt;next = nullptr; return new_head; } }; 执行用时分布8ms消耗内存分布8.73MB 92. 反转链表 II # 思路：\n大致思路和上面一样，不过该题中加入了翻转的边界限制。\n因此我们分成两个步骤来解决，保留前部分链表，以 left 为分割点分割链表，将后面的链表的前 right-left 个节点进行翻转操作。\n步骤一: 递推到第 left 个节点, 然后按顺序回归。\n步骤一终止条件：当 left==1 时停止，返回处理后的下半部分链表。\n步骤二：对于截断出的链表递推到第 right-left 个节点，在回归时进行翻转。\n步骤二终止条件：为了定位距离边界 right 还有多少个节点，我们在每次递推的时候令 right--，当 right==1 时停止递推，截断并保存next head，返回当前的 head，向前回归。\nclass Solution { private: ListNode* last_head = nullptr; ListNode* reverseRight(ListNode* head, int right){ if(right==1){ last_head = head-\u0026gt;next; return head; } ListNode* new_head = reverseRight(head-\u0026gt;next, right - 1); head-\u0026gt;next-\u0026gt;next = head; head-\u0026gt;next = last_head; return new_head; } public: ListNode* reverseBetween(ListNode* head, int left, int right) { if(left==1){ return reverseRight(head, right); } head-\u0026gt;next = reverseBetween(head-\u0026gt;next, left - 1, right - 1); return head; } }; 779. 第K个语法符号 # 思路：\n第 $i+1$ 行中的第 $x$ 个数字 $\\textit{num}_1$，$1 \\le x \\le 2^i$，会被第 $i$ 行中第 $\\lfloor \\frac{x + 1}{2} \\rfloor$ 个数字 $\\textit{num}_2$ 生成。且满足规则：\n当 $\\textit{num}_2 = 0$ 时，$\\textit{num}_2$ 会生成 $01$： $$ \\textit{num}_1 = \\begin{cases} 0, \u0026amp; x \\equiv 1 \\pmod{2} \\ 1, \u0026amp; x \\equiv 0 \\pmod{2} \\ \\end{cases} $$ ( $\\equiv$ 表示恒等于，$\\pmod{2}$ 表示对2取模)\n当 $num_2 = 1$ 时，$\\textit{num}_2$ 会生成 $10$： $$ \\textit{num}_1 = \\begin{cases} 1, \u0026amp; x \\equiv 1 \\pmod{2} \\ 0, \u0026amp; x \\equiv 0 \\pmod{2} \\ \\end{cases} $$\n并且进一步总结我们可以得到：$\\textit{num}_1 = (x \\And 1) \\oplus 1 \\oplus \\textit{num}_2$ ，其中 $\\And$ 为「与」运算符， $\\oplus$ 为「异或」运算符( 两个位相同为0，相异为1 )。那么我们从第 $n$ 不断往上递归求解，并且当在第一行时只有一个数字，直接返回 $0$ 即可。\nclass Solution { public: int kthGrammar(int n, int k) { if(n == 1){ return 0; } return (k \u0026amp; 1) ^ 1 ^ kthGrammar(n - 1, (k + 1) / 2); } }; ","date":"14 December 2023","externalUrl":null,"permalink":"/posts/leetcode/datawhale%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%9F%BA%E7%A1%80%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E9%80%92%E5%BD%92/","section":"Posts","summary":"","title":"Datawhale数据结构基础课程笔记-(Day3-6)递归","type":"posts"},{"content":" 挂载硬盘 # 查看磁盘名称等信息\nlsblk 可以看到我要挂载硬盘的名称是/dev/sda1\n挂载分区\nmkdir /test mount /dev/sda1 /test lsblk 这样就挂载成功，但是重启系统就需要重新挂载，这个时候我们需要开机自动挂载\n参考: Ubuntu 磁盘挂载——开机自动挂载\n查询挂载硬盘UUID\nsudo blkid /dev/sda1 返回信息\n/dev/sda1: LABEL=\u0026#34;data\u0026#34; BLOCK_SIZE=\u0026#34;512\u0026#34; UUID=\u0026#34;0AA6352E62EA7D6B\u0026#34; TYPE=\u0026#34;ntfs\u0026#34; PARTUUID=\u0026#34;19a435b2-37b3-4229-9728-29c163e54b74\u0026#34; 可以看到UUID=\u0026quot;0AA6352E62EA7D6B\u0026quot;, TYPE=\u0026quot;ntfs\u0026quot; 修改/etc/fstab文件 在文档末尾添加新磁盘信息\nUUID=0AA6352E62EA7D6B /media/4T ntfs defaults 0 2 第一个数字：0表示开机不检查磁盘，1表示开机检查磁盘； 第二个数字：0表示交换分区，1代表启动分区（Linux），2表示普通分区 我挂载的分区是在WIn系统下创建的分区，磁盘格式为ntfs ","date":"14 December 2023","externalUrl":null,"permalink":"/posts/linux/linux%E6%8C%82%E8%BD%BD%E7%A1%AC%E7%9B%98/","section":"Posts","summary":"","title":"linux挂载新硬盘","type":"posts"},{"content":" 端口信息查看 # 查看当前所有tcp端口情况\nnetstat -ntlp linux查看所有2541端口占用情况\nnetstat -tunlp | grep 2541 输出： tcp 0 0 192.168.50.94:2541 0.0.0.0:* LISTEN 3882497/python3 3882497 是该IP地址对应的的进程PID，通过该进程号可以很容易的找到该进程的相关的所有信息，只需要通过下面的这个命令：\ncd /proc/3882497 简单说明一下上面的 -tunlp这几个参数的意义\n-t (tcp) 仅显示tcp相关选项 -u (udp)仅显示udp相关选项 -n 拒绝显示别名，能显示数字的全部转化为数字 -l 仅列出在Listen(监听)的服务状态 -p 显示建立相关链接的程序名 当 python 程序在异常退出且占用了一个端口时，可以通过上面的方法定位到相应进程，然后kill掉\nkill -9 3882497 使用 Python socket 库在两个机器间TCP通讯 # 使用 Python socket 库在两个机器间通讯时，客户端和服务端的 ip 都要指向服务端，并保持端口号一致。\n可以测试两台主机之间是否能相互ping通。 运行时，要首先启动服务端，再启动客户端。\n端口开启/关闭 # 两台机器进行socket通信时，可能在连接时出现错误：\nConnect error: No route to host(errno:113) 出错原因：server端的防火墙设置了过滤规则\n一种解决办法：使用iptables关闭server端的防火墙 # 1.暂时关闭\nsudo service iptables stop 2.打开\nsudo service iptables start 3.永久打开和关闭\nsudo chkconfig iptables on/off 但是可能在执行时报错\nFailed to start iptables.service: Unit iptables.service failed to load: No such file or directory. 或 Failed to stop iptables.service: Unit iptables.service not loaded. 原因是在 CentOS 7 或 RHEL 7 或 Fedora 中防火墙由 firewalld 来管理，\n为了使用iptables，执行命令：\nsystemctl stop firewalld systemctl mask firewalld 安装iptables-services：\nyum install iptables-services 设置开机启动：\nsystemctl enable iptables iptables 相关操作命令：\nsystemctl stop iptables systemctl start iptables systemctl restart iptables systemctl reload iptables 保存设置：\nservice iptables save OK，再试一下应该就好使了\n开放某个端口 在/etc/sysconfig/iptables里添加\n-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 8080 -j ACCEPT 一种解决办法：使用firewalld关闭server端的防火墙 # 如果要添加范围例外端口 如 1000-2000 语法命令如下：启用区域端口和协议组合\nfirewall-cmd [--zone=\u0026lt;zone\u0026gt;] --add-port=\u0026lt;port\u0026gt;[-\u0026lt;port\u0026gt;]/\u0026lt;protocol\u0026gt; [--timeout=\u0026lt;seconds\u0026gt;] 此举将启用端口和协议的组合。端口可以是一个单独的端口 或者是一个端口范围 - 。协议可以是 tcp 或 udp。 实际命令如下：\n添加 firewall-cmd --zone=public --add-port=80/tcp --permanent （--permanent永久生效，没有此参数重启后失效） firewall-cmd --zone=public --add-port=1000-2000/tcp --permanent 重新载入 firewall-cmd --reload 查看 firewall-cmd --zone=public --query-port=80/tcp 删除 firewall-cmd --zone=public --remove-port=80/tcp --permanent ","date":"14 December 2023","externalUrl":null,"permalink":"/posts/linux/linux%E7%AB%AF%E5%8F%A3%E7%9B%B8%E5%85%B3/","section":"Posts","summary":"","title":"linux端口通信相关","type":"posts"},{"content":" 枚举算法（Enumeration Algorithm）：也称为穷举算法，指的是按照问题本身的性质，一一列举出该问题所有可能的解，并在逐一列举的过程中，将它们逐一与目标状态进行比较以得出满足问题要求的解。在列举的过程中，既不能遗漏也不能重复。\n1. 两数之和 # 思路：\n根据枚举的思想，我们需要遍历数组，并建立目标函数： $$ array[x] + array[y] = target $$ 我们从头遍历 x 和 y，直到满足上面等式后返回 x，y：\nclass Solution { public: vector\u0026lt;int\u0026gt; twoSum(vector\u0026lt;int\u0026gt;\u0026amp; nums, int target) { vector\u0026lt;int\u0026gt; ret; for(int x = 0; x \u0026lt; nums.size(); x++){ for(int y = 0; y \u0026lt; nums.size(); y++){ if(x != y \u0026amp;\u0026amp; nums[x] + nums[y] == target){ ret.push_back(x); ret.push_back(y); break; } } if(ret.size() == 2){ break; } } return ret; } }; 执行用时分布 844ms\n消耗内存分布 10.33MB\n204. 计数质数 # 思路：\n质数是只能被自己整除的自然数（0，1除外）。 根据枚举的思想，我们从2开始遍历x，每次再从2开始遍历一次y，看y是否能被x整除，以判断是否为质数，是则计数加一。 class Solution { public: int countPrimes(int n) { int count = 0; bool is_prime = true; for(int x = 2; x \u0026lt; n; x++){ for(int y = 2; y \u0026lt; x; y++){ if(x % y == 0){ is_prime = false; break; } is_prime = true; } if(is_prime){ count++; } } return count; } }; 但这样在n = 499979时会超出时间限制。因此枚举方法在此题不推荐使用。 1925. 统计平方和三元组的数目 # 思路：\n根据枚举的思想，我们设置三重循环：从1开始遍历a，再嵌套从1开始遍历b，再再嵌套从1开始遍历c，判断是否满足等式。\nclass Solution { public: int countTriples(int n) { int count = 0; for(int a = 1; a \u0026lt; n + 1; a++){ for(int b = 1; b \u0026lt; n + 1; b++){ for(int c = 1; c \u0026lt; n + 1; c++){ if(a*a + b*b == c*c){ count++; } } } } return count; } }; 执行用时分布352ms 消耗内存分布6.15MB\n时间复杂度：$O(n^3)$。 空间复杂度：$O(1)$。\n对上面的思路进一步优化。我们可以在每次改变a，b的值后直接根据公式计算c的值，通过判断c的值是否小于等于n来判断等式是否成立，从而减少一次循环嵌套。\n注意：在计算中，为了防止浮点数造成的误差，并且两个相邻的完全平方正数之间的距离一定大于 $1$，所以我们可以用 $\\sqrt{a^2 + b^2 + 1}$ 来代替 $\\sqrt{a^2 + b^2}$。\nclass Solution { public: int countTriples(int n) { int count = 0; for(int a = 1; a \u0026lt; n + 1; a++){ for(int b = 1; b \u0026lt; n + 1; b++){ int c = sqrt(a*a + b*b + 1); if(c \u0026lt;= n \u0026amp;\u0026amp; a*a + b*b == c*c){ count++; } } } return count; } }; 执行用时分布8ms 消耗内存分布6.17MB\n时间复杂度：$O(n^2)$。 空间复杂度：$O(1)$。\n2427. 公因子的数目 # 思路：\n如果 x 可以同时整除 a 和 b ，则认为 x 是 a 和 b 的一个 公因子 。 可以从1开始，遍历到a、b中较小的一个值，每次判断是否满足整除等式。\nclass Solution { public: int commonFactors(int a, int b) { int count = 0; for(int i = 1; i \u0026lt; min(a, b) + 1; i++){ if(a % i == 0 \u0026amp;\u0026amp; b % i == 0){ count++; } } return count; } }; 执行用时分布4ms 消耗内存分布6.38MB\nLCR 180. 文件组合 # 思路：\n根据题意, 大于等于 target 的数显然不需要考虑，我们从零开始遍历到目标数字target. 每次遍历时维护一个列表并将当前遍历的值x作为首元素。\n每次遍历中，我们查找以x开头并使列表元素和为target的其他元素，从x开始遍历到target，将每次遍历的值y加入到列表中，如果列表元素和超过了target，则退出遍历。\nclass Solution { public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; fileCombination(int target) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; ret; for(int x = 1; x \u0026lt; target; x++){ vector\u0026lt;int\u0026gt; temp; int sum = 0; for(int y = x; y \u0026lt; target; y++){ temp.push_back(y); sum += y; if(temp.size() \u0026gt; 1 \u0026amp;\u0026amp; sum == target){ ret.push_back(temp); break; } } } return ret; } }; 这种方法，当target =50252时会超出时间限制。\n由于组合长度需要大于2， 所以遍历条件可以改成target/2，但还是会超出时间限制。我们在第二层循环中令 sum \u0026gt; target 时直接 break，测试通过。\n输入 target = 93 输出 [[13,14,15,16,17,18],[30,31,32]] 预期结果 [[13,14,15,16,17,18],[30,31,32],[46,47]] 原因是在C++中，整数除法的结果会舍弃小数部分，保留整数部分。 target 是 int 类型的，此时 target/2 等于 63（向下取整），而不是 63.5. 我们将 x 的判定条件改成x \u0026lt;= target/2 即可。\nclass Solution { public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; fileCombination(int target) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; ret; for(int x = 1; x \u0026lt;= target/2; x++){ vector\u0026lt;int\u0026gt; temp; int sum = 0; for(int y = x;; y++){ temp.push_back(y); sum += y; if(temp.size() \u0026gt; 1 \u0026amp;\u0026amp; sum == target){ ret.push_back(temp); break; } else if(sum \u0026gt; target){ break; } } } return ret; } }; 执行用时分布328ms 消耗内存分布96.30MB\n2249. 统计圆内格点数目 # 给你一个二维整数数组 circles ，其中 circles[i] = [xi, yi, ri] 表示网格上圆心为 (xi, yi) 且半径为 ri 的第 i 个圆，返回出现在 至少一个 圆内的 格点数目 。\n思路：\n判定一个点是否在圆内，只需要判定这个点到圆心的距离是否小于等于半径。 由于 1 \u0026lt;= xi, yi \u0026lt;= 100，可以使用暴力枚举法来解决这个问题，即遍历网格中的每个点，对上述不等式作判定。 class Solution { public: int countLatticePoints(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; circles) { int count = 0; for(int x = 0; x \u0026lt; 100; x++){ for(int y = 0; y \u0026lt; 100; y++){ if(point_in_circle(x, y, circles)){ count++; } } } return count; } bool point_in_circle(int x, int y, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; circles){ for(int i = 0; i \u0026lt; circles.size(); i++){ if((x-circles[i][0])*(x-circles[i][0]) + (y-circles[i][1])*(y-circles[i][1]) \u0026lt;= circles[i][2]*circles[i][2]){ return true; } } return false; } }; 第 51 个测试用例发生报错。原因是需要x，y表示圆心的最大坐标，我们在网格中遍历时的最大坐标值需要乘以2. class Solution { public: int countLatticePoints(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; circles) { int count = 0; for(int x = 0; x \u0026lt;= 200; x++){ for(int y = 0; y \u0026lt;= 200; y++){ if(point_in_circle(x, y, circles)){ count++; } } } return count; } bool point_in_circle(int x, int y, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; circles){ for(int i = 0; i \u0026lt; circles.size(); i++){ if((x-circles[i][0])*(x-circles[i][0]) + (y-circles[i][1])*(y-circles[i][1]) \u0026lt;= circles[i][2]*circles[i][2]){ return true; } } return false; } }; 执行用时分布1008ms 消耗内存分布8.13MB 以上，第1-2天的学习题目已经作答完毕。不过，每个题目都有更优的解法，我们还需要不断地学习。\n","date":"12 December 2023","externalUrl":null,"permalink":"/posts/leetcode/datawhale%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%9F%BA%E7%A1%80%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E6%9E%9A%E4%B8%BE/","section":"Posts","summary":"","title":"Datawhale数据结构基础课程笔记-(Day1-2)枚举","type":"posts"},{"content":" 传统的强化学习算法都局限在单个目标上，训练好的算法难以完成这个目标外的其他目标。以机械臂操作为例，采用单一策略只能训练抓取同一个位置的物体。对于不同的目标位置，要训练多个策略。另外这类环境通常依赖于人工进行奖励塑形，而设计一个好的奖励函数通常很困难。因此最近的研究更希望使用稀疏的奖励，只有当成功完成任务后才能够获得奖励，在其他的时刻将不会得到奖励。然而智能体在探索过程中，大部分时间内都无法完成任务，导致有价值的样本数目非常少，智能体需要更多的时间进行探索，导致指数级的样本复杂度。\nUniversal Value Function Approximators # 值函数 $V(s)$ 用于表示状态 s 在实现 agent 总体目标或奖励函数时的效果。从定义来说，值函数量化了某个策略积累某种报酬的能力。从另一个角度思考，值函数量化了一个agent完成某个任务的能力。（摘录自UVFA, HER and Inverse RL）\n在一个环境中，任务（目标）可能是不同的，这需要不同的最优值函数去量化完成不同任务的方案。本文提出了一种单个、一致的值函数逼近器 UVFA，在原始的值函数 $V(s)$、$Q(s,a)$ 的基础上增加 goal 作为输入，变成 $V(s,g)$、$Q(s,a,g)$，这样值函数就变成在某一状态（或状态-动作）某一目标下的价值。\n另外，本文将目标定义成了最简单的一种情况：$\\mathcal{G} ⊆ \\mathcal{S}$，当状态达到目标时获得一个二进制的奖励 1。\n一般来说，智能体只会看到状态和目标（s，g）的可能组合的一小部分。为了使智能体能够概括到其余组合，上图给出了两种实现 UVFA 的网络结构。\nConcatenated 结构. 直接连接 state 和 goal 作为联合输入，可以用 MLP 实现输出到回归目标的映射； Two-Stream 结构. 分别将状态 s 和目标 g 输入到各自的网络，映射到 n 维嵌入向量空间，得到对应的两个 embedding vectors，通过某种函数 h 映射成一个标量作为回归输出，文中采用的方法是向量内积。 监督学习 # 对于 Two-Stream 结构，UVFA的训练包括两个步骤：\n、通过某种方法获取各个状态目标对应的价值（文中直接给定），然后引入了一种新的因子分解方法——低秩矩阵分解，将回归分解为两个阶段: 第一阶段，我们将数据视为一个稀疏的值表，每个观察到的状态 s 对应一行，每个观察到的目标 g 对应一列，并将表分解为状态嵌入 $φ(s)$ 和目标嵌入 $φ(g)$ ，将 $φ(s)$ 表示为 $s$ 行的目标嵌入向量，将 $φ(g)$ 表示为 $g$ 列的目标嵌入向量。 第二阶段，分别使用标准回归技术（例如梯度下降）学习从状态 $s$ 到状态嵌入 $φ(s)$ 以及从目标 $g$ 到目标嵌入 $φ(g)$ 的非线性映射。 将两个嵌入向量通过向量内积整合成一个状态目标值。 强化学习 # 对于强化学习，文中提供了两种直接从奖励中学习 UVFA 的算法。\n第一种算法使用一种 Horde of demons 的方式，可以产生不同目标对应的状态目标值，并使用这些值来 seed the table，然后通过低秩分解的方式将其分解为状态对应的嵌入变量和目标对应的嵌入隐变量，从而学习 UVFA $V(s，g; θ)$，该 $V(s，g; θ)$ 能够概括到以前看不见的目标。\n第二种算法采用了自举（bootstrapping）的思想，直接从后续状态的 UVFA 值进行更新： $$ Q(s_t, a_t, g) ← α(rg + γ_g \\underset{a\u0026rsquo;}{max} Q(s_{t+1}, a\u0026rsquo;, g))+(1 − α)Q(s_t, a_t, g) $$\n总结 # (摘录自分层强化学习survey)\n本文提出的统一值函数概念具有迁移意义，同时属于goal-reach范畴，但局限是文中提到比较困难的仍是goal如何选取，但本文并不打算讨论这个问题，因为本文的重点是提供这种考虑目标在内的广义值函数的概念，于是只是简单地选择某些state作为goal，可见goal如何合理选取将是这类分层问题的最大困难。\n另一篇写的非常详细的博客：对Reinforcement Learning中的小无相功和九阳神功的一些参悟\nHindsight Explerience Replay（HER) # HER 是建立在 UVFA 基础上的，原理是回放每个具有任意目标的轨迹。当智能体在一个回合中没有达成目标时，可以使用另外的目标来替代原来的目标，之后根据这个新的目标重新计算奖励，来为智能体提供经验。HER 改善了稀疏奖励下 DRL 的采样效率，可以将该方法与任意的 off-policy 算法结合。\n具体地，在 HER 方法下的马尔可夫决策过程表述为一个五元组，其中的状态 $S$ 不仅包含观测 $S_o$，还包括一个期望目标 $S_{dg}$， 且 $S_{dg} \\in S_o$。\n实现过程 # 实现 HER 算法的关键是构建 replay buffer，包含以下两步：\n初始化一个固定长度的队列，每次进入一个五元组 $(s_t,a_t,r_t,s_{t+1},g)$，定义初始状态 $s_0$ 和目标 $g$, 智能体根据当前状态 $s_t$ 和目标 $g$ 来采取动作。 奖励的计算如下： $$ r_t \\gets r(a_t,s_t||g) $$\n采样出的 $(s_t,a_t,r_t,s_{t+1},g)$ 将被存放到 replay buffer 中。之后，每次都会基于实际目标 $g$ 采样一段完整的智能体经验序列。 2. 使用新的目标 $g\u0026rsquo;$ 重新计算奖励：\n$$ r\u0026rsquo; \\gets r(a_t,s_t||g\u0026rsquo;) $$\n我们构建新的 transitions $(s_t, a_t, r′, s_{t+1}, g′)$ ，也存放到 replay buffer 中。\n文章给出了四种方法去重新获得新的目标 g':\nfinal: 将每个回合最后的状态作为新目标 future（效果最好）: 随机选择 k 个在这个轨迹上并且在当前transition之后的状态作为新目标 episode: 每次选择 k 个在这个轨迹上的状态作为新目标 random: 每次在所有出现过的状态里面选择 k 个状态作为新目标 一般我们使用 future 方法，因为它能够更好地利用经验。\n局限性-INNR问题 # （这里引用[4]中的文字）。\nHER 方法可以通过目标重标记策略，产生足够数量的非负奖励样本，即使智能体实际上没有完成任务。然而，在具有稀疏奖励的复杂顺序物体操作任务中(智能体必须按顺序成功完成每个子任务，以达到期望的最终目标)，由于隐含的 一致非负奖励（Identical Non-Negative Reward，INNR） 问题，智能体仍然会受到样本效率低下的困扰。当智能体在探索过程中无法影响已实现目标时，就会出现INNR问题。在这种情况下，智能体无法从相同负奖励的原始样本，或一致非负的虚拟样本中，区分哪个动作更好。换句话说，实际探索的样本都是负样本，目标重标记的虚拟样本都是正样本，因此并不能从这些样本中区分好坏。因此，来自HER的INNR对于策略改进几乎没有帮助，甚至会降低策略的探索能力。这个隐含的INNR问题是HER在标准操作任务中样本效率低下的原因。\n例如，在 Push 任务中，智能体必须先接近物体，然后将其推到期望的位置。当智能体在一个episode中未能改变物体的位置时，所有的已实现目标在整个episode 中是相同的。在经验回放过程后，所有的事后目标也将与已实现目标具有相同的值，导致所有的事后样本都是成功的。然而，这样的成功并不是由智能体的行动造成的。这些样本对智能体的策略改进没有帮助，甚至阻碍了学习。\nReferences # Hindsight Experience Replay (neurips.cc) 【强化学习算法 34】HER - 知乎 (zhihu.com) [强化学习5] HER（Hindsight Experience Replay） - 知乎 (zhihu.com) [2208.00843] Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards (arxiv.org) baselines/baselines/her at master · openai/baselines (github.com) openai 使用 her 在机器人操作上的应用研究： ingredients-for-robotics-research Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research GCRL with Sub-goal Selection / Generation # 本文翻译与参考自: Goal-Conditioned Reinforcement Learning: Problems and Solutions\n由于难以实现长期目标，可以把目标分割成许多短期目标来帮助智能体到达长期目标。 在期望目标被替换后收集经验，就是将环境的目标分布从$f$改成$p_g$，目标函数变成\n$$ J(\\pi)=\\mathbb{E}_{\\textstyle{s_{t+1}\\sim \\mathcal{T}(\\cdot\\mid s_t,a_t),\\atop a_{t}\\sim \\pi(\\cdot\\mid s_{t},g),g\\sim f }}\\left[\\sum_t \\gamma^tr(s_t,a_t,g)\\right] $$\n$f$可以是一个规则，能够从过去的经验中选择$g$，或者是根据某种准则学习出的一个函数。\n下面介绍几种选择子目标的方法。\n1. 中间难度 Intermediate difficulty # 准确评估到达目标的难度有助于确定在智能体的能力范围内学习什么，一个合理的目标应该超出智能体的范围（why？）\nAutomatic goal generation for reinforcement learning agents. 根据奖励给难度打分，用来把目标标记成正/负采样，然后训练一个GAN模型来生成具有合理难度分数的目标。 Learning with amigo: Adversarially motivated intrinsic goals. 提出了一种 teacher-student 框架。teacher充当目标提供者，尝试提供难度递增的目标。teacher从student在提供的目标上的表现中学习，student从环境中得到的外部奖励和teacher中得到的内部奖励学习。 Automatic curriculum learning through value disagreement. 把Q函数认知的不确定性看成是实现目标难度的衡量，用于构建一个分布来采样目标。当不确定性过高时，目标应该处于策略的知识边界，以一个合理的难度来学习。 2. 探索意识 Exploration awareness # 由于稀疏奖励限制了策略的学习效率，我们应该提高策略的探索能力，尽可能覆盖没有遇到的目标和状态。 在GCRL，许多研究者们提议使用子目标采样来得到一个更好的探索能力。\nMaximum entropy gain exploration for long horizon multi-goal reinforcement learning. 提出采样使过去已实现目标的熵最大化的目标，以提高目标空间（goal space）的覆盖范围。（这一点和SAC的目的很像） State-covering self-supervised reinforcement learning. 训练一个自监督的生成式模型来生成行为目标，该目标在过去的经验中对已实现目标进行了近似均匀的倾斜，表现为最大化期望目标分布的熵。 Unsupervised controlthrough non-parametric discriminative rewards. 选择从不同的replay buffer中均匀地采样过去已实现的目标作为行为目标，这些储存的目标间的距离要尽可能地远。 Visual reinforcement learning with imagined goals. 选择从经验回放中均匀地采样过去已实现的目标，用这些历史目标拟合生成式模型来生成行为目标。 Exploration via hindsight goal generation 通过 Wasserstein Barycenter 问题，生成了一组事后目标以进行探索。把生成的目标作为隐性课程，可以有效地使用当前值函数来学习。 Dynamical distance learning for semi-supervised and unsupervised skill discovery 学习一个距离函数，并从replay buffer中选择距离初始状态最远的目标来提高探索效率。 3. 从经验中搜索 Searching from experience. # 历史经验包含导向实现某些目标的路径点，可以使人类或智能体获益。\nHindsight planner. 从一个采样的回合中选择 $k$ 个关键的中间路径点，并训练一个RNN网络，根据给定的起始和终止点来生成序列。再应用这些中间点作为期望目标的序列来与环境交互。 Search on the replay buffer: Bridging planning and reinforcement learning. 在 replay buffer 上建立图，把状态作为节点，从起始状态到目标状态的距离作为边的权重。该工作利用了二值奖励下的状态值可以近似成到目标的距离这一直觉，使用图搜索去寻找路径点序列来实现目标，并不断通过在路径点上学习的策略来采取动作。 4. Model-based planning 和 Learn from experts. # model-based RL 与 learning from demonstrations 不在笔者的考虑范围内，故不在此翻译。\nRelabeling in GCRL # 与子目标选择类似，relabeling也替换了初始的期望目标以提高学习效率。二者的区别是：\nRelabeling 关注于在训练前替换经验容器中的历史数据； Sub-goal Selection 关注于改变采样经验的分布。 为了实现重标记操作，理论上需要一个重标记函数 $h$ 去替换从 replay buffer 采样出的 transition 中的目标，并重新计算奖励 $$ (s_t, a_t, g_t, r_t) \\gets (s_t, a_t, h(·), r(s_t, a_t, h(·))) $$ 1. 事后回放 Hindsight relabeling. # 该方法可以追溯到最著名的 Hindsight experience replay(HER)，想法源自于人类从失败的经验中来学习。HER 使用replay buffer相同轨迹中已实现目标来替换期望目标，同时减轻稀疏奖励的问题。\nCurriculum-guided hindsight experience replay. 提出 CHER，使用课程重标记方法自适应地从失败经验中选择重标记目标。课程的评价标准是与期望目标的相似度和目标的多样性。\nGoal densitybased hindsight experience prioritization for multi-goal robot manipulation reinforcement learning. 为已实现目标学习了一个稠密模型，用于优先标记少见的目标。以实现在少见经验中探索来提高采样效率。\n2. Relabeling by learning. # Guided goal generation for hindsight multigoal reinforcement learning 从过去的经验选择重标记目标限制了重标记目标的多样性，为了减轻该问题并能够重标记未见过的目标，这篇文章通过隐式地建模策略表现和已实现目标之间的关系来训练一个条件生成式RNN网络，用于根据当前策略的平均返回值生成重标记目标。 3. 先见之明 Foresight. # 人类不仅可以从失败中学习，还可以基于当前状态对未来作出规划。\nMapgo: Modelassisted policy optimization for goal-oriented tasks. 学习一个动态模型用来生成用于重标记的虚拟轨迹。先见目标重标记可以防止标记局限于历史数据的同质目标，规划出当前策略可以实现的新目标。 ","date":"21 November 2023","externalUrl":null,"permalink":"/posts/algorithm/5-gcrl/","section":"Posts","summary":"","title":"Goal-Conditioned RL","type":"posts"},{"content":" 前言 # 最近花了一些阅读和复现 Relay Hindsight Experience Replay（RHER） 算法，这篇文章写的非常扎实，但作者开源的代码是基于 tensorflow 的 baselines 框架编写的。我在使用 pytorch 重写的过程中有了一些心得体会，在这分享出来。\n原文传送门：Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards\nRHER 的 GITHUB 链接：https://github.com/kaixindelele/RHER\n我自己使用 pytorch 复现的 GITHUB 链接：https://github.com/NoneJou072/rl-notebook\n非常感谢作者的工作。\n复现过程 # issue-1 # 作者在 baselines 中的更新策略的方法是，按顺序分别训练 push 子任务和 reach 子任务。在 issue-5 中也提到了 每次更新当前的，以及下一个任务的策略函数。但在我的复现过程中发现，这样会出现 灾难性遗忘，当更新下一个任务的策略时，上一次被更新的任务会被遗忘掉。\n但是作者又在 issue 中回答到：\n不能存轨迹，而是每次交互后，先目标重标记，再把所有的子任务的数据，分别存到对应的经验池中。\n论文中也提到：\n对于三物体任务，需要更新先前的部署细节：1）使用基于PyTorch的双延迟DDPG（TD3）方法[47]，而不是基于TensorFlow的DDPG方法。\n对于基于PyTorch框架的多物体操作任务，在每收集一次轨迹后需要立即重新标记，并将多个子任务的transition，分别存储在不同的经验池中。在每次更新时，样本独立收集，然后组合成一个完整的批次。\n一个令我比较疑惑的点是，为什么三物体不接着延续基于 tensorflow 的 baselines 框架，为什么使用 baselines 框架时按顺序更新策略函数不会出现遗忘问题。\nreproduction-1 # 于是，我使用 pytorch 的 DDPG 方法，按照上述的描述进行了修改，并加入了一些我自己的调整，大概的伪代码为\n分别为 push 和 reach 子任务创建两个经验池 与环境交互，得到一个 episode 的 transitions 回合结束后，分割出当前 episode 属于 reach 子任务的 transitions。之后分别将轨迹存到对应的经验池中。 策略更新时，分别从两个经验池中采样，然后组合成一个完整的批次。 issue-2 # 使用基于 pytorch 编写的 DDPG 方法进行训练时，发现奖励函数的值会先抵达 0.8 左右的成功率，之后便会降低到 0.4 左右。\n在 DDPG 中，actor loss 表示负的 Q 值，actor loss 越小，Q 值越大。此时 actor loss 呈现出很大的下降趋势，critic loss 呈现很大的震荡。明显是出现 Q 值被高估的情况了。\n为什么会有Q值高估问题出现？-\u0026gt; 曾伊言: 强化学习算法TD3论文的翻译与解读。\n由于 Q 值过大，会导致时序差分目标 q_targets 过大\n参考自 Y. F. Zhang: 一个AC类算法策略loss引出的思考\n结果对比 # 下面是作者在 FetchPush-v1 环境下的结果：\n下面是我在 FetchPush-v2 环境下的复现结果：\n可以看到，在训练时间 56min 左右时，成功率稳定在了 95% 左右，与作者的结果相差不大。\n","date":"24 October 2023","externalUrl":null,"permalink":"/posts/algorithm/4-rher/","section":"Posts","summary":"","title":"Relayed HER复现结果与分析","type":"posts"},{"content":" geom # contype: int, “1” / conaffinity: int, “1” # contype 和 conaffinity 指定用于动态生成接触对的32位整数位掩码（请参见 Computation 章节中的碰撞检测）。如果一个geom的contype与另一个geom的conaffinity 是 \u0026ldquo;compatible\u0026rdquo; 的，则两个geom可以发生碰撞。\n\u0026ldquo;compatible\u0026ldquo;意味着两个位掩码具有一个公共位设置为1。即： (contype1 \u0026amp; conaffinity2) || (contype2 \u0026amp; conaffinity1) 的结果布尔值为 True。\n由于默认值 contype = conaffinity = 1，因此平时可以忽略该参数的设置。\ncondim: int, “3” # 对于动态生成的接触对的接触空间的维度，设置为两个 geoms 中最大 condim 的值 （参考 Computation 章节中的 Contact）。\n下面是可用的值和对应的含义:\ncondim Description 1 Frictionless contact. 3 Regular frictional contact, opposing slip in the tangent plane. 4 Frictional contact, opposing slip in the tangent plane and rotation around the contact normal. This is useful for modeling soft contacts (independent of contact penetration). 6 Frictional contact, opposing slip in the tangent plane, rotation around the contact normal and rotation around the two axes of the tangent plane. The latter frictional effects are useful for preventing objects from indefinite rolling. joint # armature: real, “0” # 各自由度的 armature inertia (电机转子惯性) (or rotor inertia, or reflected inertia)。\n在广义坐标中，作为常数被加到惯性矩阵的对角线上，使仿真更稳定，并增加了物理真实性。这是因为当电机与系统相连时，传动装置会将电机力放大 c，转子（即电机的运动部分）的惯性也会放大 c*c。行星齿轮箱早期阶段的齿轮也是如此。这些额外的惯性通常在模型中明确表示的机器人部件的惯性中占主导地位，而电枢属性正是对它们进行建模的方法。\n","date":"23 September 2023","externalUrl":null,"permalink":"/posts/5-mujoco%E9%83%A8%E5%88%86%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E/","section":"Posts","summary":"","title":"Mujoco 参数说明","type":"posts"},{"content":" 介绍 # 使用三维软件建模后，发现导出的mesh文件是由三角形组成的面，那么如何将该 mesh 转成由四边形组成的面呢？可以使用 Blender 来完成这一操作。\n我们需要安装一个 Blender 插件：jremesh-tools，直接在 github 选择下载 ZIP 压缩包。\n参照提供的部署流程，我们再去下载 instant-meshes，可以直接下载编译好的二进制文件-Pre-compiled binaries(windows)。\n然后，将 jremesh-tools 插件导入到 blender 中：\n插件安装流程：Blender-编辑-偏好设置-插件-安装-选择下载好的zip压缩包-安装插件-勾选单选框 之后，点开单选框左边的箭头，在偏好设置中，将之前下载好的 instant-meshes 的二进制文件 Instant Meshes.exe 添加到路径中。\n添加完路径后，在 Blender 主界面按 N 键，出现选项卡，选择 JRemesh\n之后，导入并选中我们的 mesh 模型，点击重构网格，即可生成由四边形面片构成的 mesh。\n如果报错 JRemesh: Path to Instant Meshes is missing ，参考 JRemesh: Path to Instant Meshes is missing. #9，是 instant-meshes 的路径没有正确索引到。\n","date":"23 September 2023","externalUrl":null,"permalink":"/posts/6-%E4%BD%BF%E7%94%A8blender%E5%B0%86%E4%B8%89%E8%A7%92%E5%BD%A2mesh%E8%BD%AC%E4%B8%BA%E5%9B%9B%E8%BE%B9%E5%BD%A2/","section":"Posts","summary":"","title":"使用Blender将三角形mesh转为四边形","type":"posts"},{"content":" 1. 注册 PyPi 账户并验证 # 1.为了上传到 PyPi，我们需要在 pypi 网站中注册一个账户，注册完会给你的邮箱发送邮件，要点开邮件中的链接来验证身份。\n之后，会跳转到二次身份验证页面(2FA)。根据指引，我们生成了几行 Account recovery codes，记得先保存下来，再点击继续，把刚才保存的安全码中的一行输入进去，即可验证成功。\n接着，我们开始进行账户的二次验证。pypi 提供了两个途径，分别是 Add 2FA with authentication application 和 Add 2FA with security device。\n我们选择 with authentication application 的方式。去谷歌商店中下载 Microsoft Authenticator，打开 app 后，选择底部的已验证ID-扫描QR码，扫描 Pypi 网页中出现的 QR 码。之后切换到 Authenticator 界面，点开 PyPi，会出现一次性密码代码，将该代码输入到 PyPi 网页中 QR 码右侧的输入栏，然后点击验证即可。\n2. 调整本地项目的文件结构 # 要打包的项目应该符合如下文件结构：\n/packaging_tutorial /example_pkg __init__.py setup.py LICENSE README.md 其中，\nsetup.py 是setuptools的构建脚本。它告诉setuptools你的包（例如名称和版本）以及要包含的代码文件。 LICENSE 文件中包含着我们的许可证，用于告诉用户安装你的软件包可以使用您的软件包的条款。有关选择许可证的帮助，请访问 https://choosealicense.com/。选择许可证后，可以将其内容拷贝到 LICENSE 文本文件中。 README.md 可以放入一些对项目的介绍。 3. 配置 setup.py # setup.py是 setuptools 的构建脚本。它告诉 setuptools 你的包（例如名称和版本）以及要包含的代码文件。下面是一个setup.py内容示例：\nimport setuptools with open(\u0026#34;README.md\u0026#34;, \u0026#34;r\u0026#34;) as fh: long_description = fh.read() setuptools.setup( name=\u0026#34;example-pkg-your-username\u0026#34;, version=\u0026#34;0.0.1\u0026#34;, author=\u0026#34;Example Author\u0026#34;, author_email=\u0026#34;author@example.com\u0026#34;, description=\u0026#34;A small example package\u0026#34;, long_description=long_description, long_description_content_type=\u0026#34;text/markdown\u0026#34;, url=\u0026#34;https://github.com/pypa/sampleproject\u0026#34;, packages=setuptools.find_packages(), classifiers=[ \u0026#34;Programming Language :: Python :: 3\u0026#34;, \u0026#34;License :: OSI Approved :: MIT License\u0026#34;, \u0026#34;Operating System :: OS Independent\u0026#34;, ], ) 对于setup()中参数的介绍如下：\nname - 包的分发名称。只要包含字母，数字_和，就可以是任何名称-。它也不能在pypi.org上使用。请务必使用您的用户名更新此内容，因为这可确保您在上传程序包时不会遇到任何名称冲突。 version - 包的版本。 author - 包的作者。 author_email 包的作者邮箱。 description - 包的一个简短的总结。 long_description - 包的详细说明。这显示在Python Package Index的包详细信息包中。在这种情况下，加载长描述README.md是一种常见模式。 long_description_content_type - 告诉索引什么类型的标记用于长描述。在这种情况下，它是Markdown。 url - 是项目主页的URL。在许多项目中是一个指向GitHub，GitLab，Bitbucket或类似代码托管服务的链接。 packages - 应包含在分发包中的所有导入包的列表。我们可以使用 find_packages() 自动发现所有包和子包。在示例中，包列表为 example_pkg，因为它是唯一存在的包。 classifiers - 关于包的其他元数据。在示例中，该软件包仅与 Python 3 兼容，根据MIT许可证进行许可，且与操作系统无关。有关分类器的完整列表，参阅 https://pypi.org/classifiers/。 除了这里提到的还有很多。有关详细信息可以参阅 打包和分发项目。\n4. 使用 MANIFEST.in 打包静态资源 # 在项目目录下新建 MANIFEST.in 文件，里面的内容参考如下示例：\ninclude *.txt #包含根目录下的所有txt文件 recursive-include examples *.txt *.py #包含所有位置的examples文件夹下的txt与py文件 prune examples/sample?/build #排除所有位置examples/sample?/build global-include * # 包含安装包下的所有文件 5. 生成分发档案 # 下一步是为包生成分发包。这些是上传到包索引的档案，可以通过pip安装。\n首先确保安装了最新版本的 setuptools 和 wheel ：\npython3 -m pip install --user --upgrade setuptools wheel 现在从与setup.py位于的同一目录运行此命令：\npython3 setup.py sdist bdist_wheel 完成后，会在生成 dist 文件夹，并包含两个文件：\ndist/ example_pkg_your_username-0.0.1-py3-none-any.whl example_pkg_your_username-0.0.1.tar.gz 6. 上传 PYPI # 接下来我们开始将项目包上传到 PYPI，首先进入 PyPi 账户设置中的 Create API token 界面，根据指引创建新的 API token，记得将生成的 token 复制下来。然后继续按照教程配置 token，具体地：\n新建或修改$HOME/.pypirc文件 (windows 中在 users`` 目录下新建 .pypirc`) 文件内容的模板如下：\n[pypi] username = __token__ password = pypi-AgEIcHlwaS5vcmcCJGYzYjQzM2NjLWFiYTMtNGM1Zi05ZWU1LTQ2MTUzOGUyNzA3NwACKlszLCJmMzkyMGY4YS05ZTVkLTQyMmEtOGE2MS1lM2IyYjJhZTNlMjgiXQAABiDQYlxiUUsdywZ2RtK__n0x0lgobwbai2ocPdFTRVG_ig 回到项目命令行，输入\nsudo apt install twine twine upload dist/* 输入 PyPI 的用户名和密码。命令完成后，应该看到与此类似的输出：\nUploading distributions to https://test.pypi.org/legacy/ Enter your username: [your username] Enter your password: Uploading example_pkg_your_username-0.0.1-py3-none-any.whl 100%|█████████████████████| 4.65k/4.65k [00:01\u0026lt;00:00, 2.88kB/s] Uploading example_pkg_your_username-0.0.1.tar.gz 100%|█████████████████████| 4.25k/4.25k [00:01\u0026lt;00:00, 3.05kB/s] 现在，我们的包已经上传到 PyPi 上了，可以进入到 projects 界面查看我们的项目。\n","date":"7 September 2023","externalUrl":null,"permalink":"/posts/4-python%E6%89%93%E5%8C%85/","section":"Posts","summary":"","title":"打包 Python 项目并上传至 PyPi","type":"posts"},{"content":"","date":"13 June 2023","externalUrl":null,"permalink":"/posts/algorithm/","section":"Posts","summary":"","title":"algorithm","type":"posts"},{"content":"","date":"13 June 2023","externalUrl":null,"permalink":"/posts/leetcode/","section":"Posts","summary":"","title":"leetcode","type":"posts"},{"content":"","date":"13 June 2023","externalUrl":null,"permalink":"/posts/linux/","section":"Posts","summary":"","title":"linux","type":"posts"},{"content":"","date":"13 June 2023","externalUrl":null,"permalink":"/posts/vla/","section":"Posts","summary":"","title":"vla","type":"posts"},{"content":" 1. 新建工程 # 文件夹名称最好和 CubeMX 项目名称一致\n2. 配置工程 # 首先把 .s 结尾的文件移动到 Core 文件夹内\n添加资源文件夹（Project Resources），把所有含有代码的文件夹都添加进去\n打开 makefile 文件，里面包含了编译信息\n","date":"29 March 2023","externalUrl":null,"permalink":"/posts/14-eide%E9%85%8D%E7%BD%AEstm32/","section":"Posts","summary":"","title":"Eide 配置 Stm32","type":"posts"},{"content":" Brief Introduction # TPE（Tree-structured Parzen Estimator），是一种基于树结构的贝叶斯优化算法，用于解决黑盒函数的全局最优化问题。\n在每次试验中，对于每个超参，TPE 为与最佳目标值相关的超参维护一个高斯混合模型 l(x)，为剩余的超参维护另一个高斯混合模型 g(x)，选择 l(x)/g(x)最大化时对应的超参作为下一组搜索值。通过这种方式，TPE 算法能够自适应地调整参数搜索空间的大小，并且能够在尽可能少的迭代次数内找到全局最优解。\n主要适用的情景：\nx 的维度不是太大，一般会限制在 d\u0026lt;20，x 可以理解为一个超参数序列 f(x) 是一个计算起来很消耗时间的函数，例如损失函数 对 f(x) 很难求导 与基于 GP 方法的区别 # GP 直接对给定输入 x 与输出 y 的后验概率分布 $p(y|x)$ 进行建模。在这种方法中，通过计算训练数据集中的数据点之间的协方差矩阵来定义一个高斯分布，从而建立起输入与输出之间的联合概率分布。这种方法的优点在于可以自适应地学习输入与输出之间的复杂关系，并且可以提供不确定性估计。\nTPE 分别对条件概率分布 $p(x|y)$ 和边缘概率分布 $p(y)$ 进行建模，然后通过贝叶斯公式来计算后验概率分布 $p(y|x)$。这种方法的优点在于可以更灵活地对不同的因素进行建模，并且可以更好地处理缺失数据和噪声。\nTree-structured # 超参数优化问题可以理解为在图结构的参数空间上不断寻找 objective function 最优解的问题。所谓 tree，是提出 TPE 的作者将该优化问题限制在了树状结构上，例如：\n一些超参数只有在其它的超参数确定后才能够进行确认，例如网络的层数与每一层的节点数量，当然这不意味着这两个超参数是相关的。实际上在 TPE 中，要求所估计的超参数必须是相互独立的。\n‍\nOptimizing EI in the TPE algorithm # TPE 使用两个密度函数来定义 $p(x|y)$：\n$l(x)$ 是使用观测空间 ${x ^{(i)}}$ 来建立的，该观测空间对应的损失 $f(x ^{(i)})$ 小于 $y^*$，使用剩下的观测来建立 $g(x)$。\n基于 GP 的方法偏向更大的 $y^$，基于 TPE 的方法取决于比当前所观测到最好的 $f(x)$ 更大的 $y^$，这样一些点就可以用于建立 $l(x)$。\nTPE 选择期望改进（expected improvement，EI）作为采集函数，由于无法得到 $p(y|x)$，我们使用贝叶斯公式进行如下转换：\n其中，$y^*$ 代表阈值，我们令 $γ = p(y \u0026lt; y^∗ )$，表示 TPE 算法的一定分位数，用于划分 $l(x)$ 和 $g(x)$，范围在(0,1)之间。\n采集函数用于确定在何处采集下一个样本点。\n为了简化上式，我们首先对分母进行构造：\n$p(x)=∫p(x∣y)p(y)dy=γl(x)+(1−γ)g(x)$\n假设我们有 100 个观测值，$\\gamma$ 设置为 0.2，意味着我们将使用 20 个最佳观测值来构建好的分布，其余的 80 个用于构建坏的分布。\n其次，对于分子，我们可以得到\n最后，EI 可以化简为\n最后一个表达式表明，为了最大化 EI，我们希望在点 x 下 $l(x)$ 概率高的同时 $g(x)$ 概率低。在每次迭代中，算法返回具有最大 EI 的候选 $x^*$:\n$x^=argmax\\ EI_{y^}(x)$\nOptimizing EI 例子 # 下面是一个例子，解释了上述迭代过程。来自 https://www.youtube.com/watch?v=bcy6A57jAwI，这个视频对 TPE 讲述得非常透彻，推荐观看。\n假设 f （蓝线）是我们要估计的黑盒函数（例如神经网络中的损失函数），我们的目的是要找出最小的损失函数值所对应的超参数。首先根据设定好的一定分位数 $\\gamma$，将我们事先采样到的一些观测分类。如下图所示，高于阈值的值意味着我们的损失过大，因此对应的点被划分到 bad group，剩下的就被划分为 good group。\n然后，分别计算 good group 和 bad group 的概率密度函数 $l(x),g(x)$。具体的计算方法在下一节会进行解释。之后，将 $l(x),g(x)$ 代入之前提到的 EI 公式，我们得到了 EI 函数的表达式。求该函数极大值所对应的参数，即期望改进最大处的超参数，将其作为新采样点进入下一次迭代，然后将这个新采样点加入 good group 或 bad group，如此反复迭代。\n（下面这个图的虚线标注有一些错误，应该和 EI 的极值点重合）\n‍\nParzen Window # 如何使用核密度估计器来分别建模黑盒函数中较差和较好参数的概率分布（即如何得到 $l(x),g(x)$），TPE 算法使用 Parzen Window 来实现。\nParzen–Rosenblatt window (Parzen 窗)是在核密度估计问题(Kernel density estimation)中，由 Emanuel Parzen 和 Murray Rosenblatt 提出的能够根据当前的观察值和先验分布类型，估计估计值的概率密度。\nParzen window 的概率密度估计公式为：\n计数函数 $\\phi(\\frac{x_i-x}{h})$ 也被称作窗函数，也可以使用高斯函数作为窗函数，即距离 $x_i$ 越近则计数权重越大：\n且满足 $\\int p(x)dx=1$。明显看出，上式与高斯混合模型（GMM）类似。因此，在有关 TPE 的大部分文章中，都将 Parzen window 当作 GMM 来解释。\n‍\nGaussian Mixture Model 和 使用高斯函数作为窗函数的 Parzen window 有什么联系？\nGaussian Mixture Model（GMM）和使用高斯函数作为窗函数的 Parzen window 是两种不同的概率密度估计方法，但它们都可以被看作是在数据集上使用一些基函数（如高斯函数）进行加权求和来估计未知概率密度函数。\n具体来说，GMM 是一种将多个高斯分布加权组合成一个更复杂分布的方法。在 GMM 中，我们假设数据是由若干个高斯分布的加权和组成的，每个高斯分布对应一个聚类中心（也称为“高斯混合成分”）。这些聚类中心的均值和协方差可以通过最大似然估计来确定。通过这种方式，GMM 可以很好地建模复杂的数据分布，因为它可以适应不同的聚类和变异度。\n与此相反，使用高斯函数作为窗函数的 Parzen window 方法是一种非参数密度估计方法。在这种方法中，我们首先选择一个核函数（如高斯函数），然后将它放在每个数据点上，计算每个数据点周围的核函数的加权和来估计该点的密度。具体来说，窗口的大小可以通过调整核函数的带宽来控制。这种方法的优点是可以适应不同的数据分布，但是由于它需要计算每个数据点周围的核函数的加权和，因此计算复杂度较高。\n需要注意的是，当高斯函数作为 Parzen 窗口的核函数时，它与 GMM 中的高斯分布非常相似。在这种情况下，高斯函数的带宽可以看作是 GMM 中高斯分布的协方差矩阵的平方根。因此，可以将 GMM 视为一种具有固定带宽的 Parzen 窗口方法。\n（回答来自 ChatGPT）\n‍\n在超参数优化中，对于不同类型的超参数，应该使用什么分布?\n类别——类别分布（Categorical Distribution）\n实数/整数——\n在原始空间采样——截断高斯混合模型 Mixture of Truncated Gaussian 在对数空间采样——对数空间的截断高斯混合模型 ‍\n为什么使用截断高斯混合模型？\n因为在随机搜索中，我们实际上指定了搜索值的范围。我们想在 TPE 中也实现这样的操作，因此将超过范围的值强制设置为 0。并且我们认为，每个点（超参数）对应一个高斯分布，所以使用截断高斯混合模型。\n**Mixture of Truncated Gaussian（截断高斯混合模型）**是一种概率分布模型，它由多个截断高斯分布组成，每个分布的参数可能不同。在这个模型中，每个高斯分布都是被截断的，也就是说，它们的密度函数在某些范围之外为零。这个范围通常是指定为一个有限的区间，超出该区间的值的概率被定义为零。截断高斯混合模型通常用于对数据集建模，特别是当数据集包含多个不同的子集，每个子集都服从不同的高斯分布时。在这种情况下，截断高斯混合模型可以有效地对数据集进行建模，并且能够捕获多个高斯分布的特征。这个模型可以应用于许多领域，例如模式识别，数据挖掘和机器学习。\n‍\nParzen Window 例子 # 假设现在有三个观测值，每个观测值对应一个高斯分布（红色虚线）\n现在按顺序将这些高斯分布的标准差设为左右邻居间的最大距离，有了不同的高斯分布，然后将这些分布混合成 single distribution（蓝色曲线）。\n在混合高斯分布中，如何计算某个点的概率密度？\n对于上述的例子，该点的概率为每个分布上概率之和除以分布数量。\n‍\n参考文献 # 贝叶斯超参数优化 Bergstra, James, et al. \u0026quot;Algorithms for hyper-parameter optimization.\u0026quot; ​Advances in neural information processing systems 24 (2011). https://en.wikipedia.org/wiki/Kernel_density_estimation Automated Machine Learning - Tree Parzen Estimator (TPE) https://blog.csdn.net/lly1122334/article/details/88345256#t7 https://blog.csdn.net/jose_M/article/details/106214842 Parzen window 密度估计——一种非参数概率密度函数估计方法 ‍\n","date":"29 March 2023","externalUrl":null,"permalink":"/posts/algorithm/7-tpe/","section":"Posts","summary":"","title":"Tree-structured Parzen Estimator（TPE）","type":"posts"},{"content":"下面使用到的参数以 skrl 库为例，不同 RL 算法库之间的参数命名可能不一样。\nrewards_shaper_scale # 在 skrl 中，可以通过设置缩放系数 rewards_shaper_scale 来对奖励进行缩放。\n推荐修改 skrl 中的 runner.py 的 reward_shaper_function，将缩放函数调整为动态计算奖励的 running mean 和 running var 对奖励归一化处理，其代码为\nclass RewardScaling: def __init__(self, shape, gamma): self.shape = shape # reward shape=1 self.gamma = gamma # discount factor self.running_ms = RunningMeanStd(shape=self.shape) self.R = np.zeros(self.shape) def __call__(self, x): self.R = self.gamma * self.R + x self.running_ms.update(self.R) x = x / (self.running_ms.std + 1e-8) # Only divided std return x def reset(self): # When an episode is done,we should reset \u0026#39;self.R\u0026#39; self.R = np.zeros(self.shape) kl_threshold # KL 散度描述了当前更新的策略和上一个策略相比的变化情况，由于 PPO 在一次迭代中使用同一批数据进行策略更新，因此不能让策略变化过大，需要对 KL 散度进行限制，通过设置一个最大kl值kl_threshold，在每次迭代中当达到这个最大kl就停止这次迭代。 对应 skrl 中的代码:\n# early stopping with KL divergence if self._kl_threshold[uid] and kl_divergence \u0026gt; self._kl_threshold[uid]: break 在 skrl 中，kl_threshold 默认设置为 0，即不使用早停。在 rl_games 中，这一参数默认设置为 0.008\n举个例子：\n情景A: 训练时如果发现 policy loss 呈震荡趋势（如下图），并且策略不收敛，可能是多种原因导致的，首先可以排查 KL 散度的问题。 观察训练时的 KL 散度，发现 KL 散度值在 0.7 左右波动，说明策略的变化过大，可以将阈值设置为 kl_threshold=0.01。 重新训练后，策略更新变得平稳许多。\n情景B: 训练时如果发现 reward 突然下降（如下图），那么一定是策略的探索行为导致了破坏性的更新。 观察训练时的 KL 散度（如下图），发现 KL 散度值在 1e-4~1e-3 区间内波动，而在某一时刻，KL 达到了 0.002，而这一时刻正对应着 reward 开始下降的时刻。 这说明，kl_threshold=0.01对于当前训练的任务来说还是太高了，策略发生稍微较大的更新就会破坏学习，可以尝试重新调整为kl_threshold=0.001。\n针对情景B，reward 突然下降这一问题，在知乎回答中【whycadi​】也提到，办法之一是“要限制最终采样的动作值的范围，减小学习率。”。这其实和早停方法的道理是一样的，我们通过早停方法来限制策略的更新，避免过度向外采样。\nLearning rate scheduler - KL Adaptive # skrl 默认使用 KLAdaptiveLR 作为学习率调度器，该调度器也接受一个名为 kl_threshold 的参数，其更新策略为:\nIF KL \u0026gt; kl_threshold * kl_factor THEN\nnext_lr = max(lr / lr_factor, min_lr)\nIF KL \u0026lt; kl_threshold / kl_factor THEN\nnext_lr = min(lr * lr_factor, max_lr)\n默认值： kl_threshold: float = 0.008, min_lr: float = 1e-6, max_lr: float = 1e-2, kl_factor: float = 2, lr_factor: float = 1.5,\n举个例子，假如设置 kl_threshold=0.008，其它参数使用默认值。 如果 KL 散度值超过阈值 0.008*2=0.016 ，学习率会开始下降，避免策略更新过猛。当小于阈值 0.008/2=0.004 时，说明策略更新幅度小，调度器会增加学习率。\nlearning_rate_scheduler_kwargs: kl_threshold: 0.008 针对情景B，也可以尝试降低该调度器中的 kl_threshold。 然而，如果kl_threshold设置的过低，会限制策略的探索能力，容易收敛到局部最优解。\ndiscount factor - gamma # 这一参数【曾伊言】已经讲解的非常清楚，因此直接将原文复制过来：\n这个值的含义是“你希望你的智能体每做出一步，至少需要考虑接下来多少步的reward？”如果我希望考虑接下来的t 步，那么我让第t步的reward占现在这一步的Q值的 0.1， 即公式 $0.1\\approx\\gamma^t$ ，变换后得到：$\\gamma\\approx 0.1^{1/t}$\ngamma ** t = 0.1 # 0.1 对于当前这一步来说，t步后的reward的权重 gamma = 0.1 ** (1/t) t = np.log(0.1) / np.log(gamma) 0.9 ~= 0.1 ** (1/ 22) 0.96 ~= 0.1 ** (1/ 56) 0.98 ~= 0.1 ** (1/ 114) 0.99 ~= 0.1 ** (1/ 229) 0.995 ~= 0.1 ** (1/ 459) 0.999 ~= 0.1 ** (1/2301) # 没必要，DRL目前无法预测这么长的MDPs过程 可以看到 0.96, 0.98, 0.99, 0.995 的gamma值 分别对应 56, 114, 229, 459 的步数 调整 gamma 的意义在于加速收敛。如果 gamma 过于大，意味着越多考虑未来的奖励，而未来的不确定性更多，超出了智能体所能掌控的范围。 gamma绝对不能选择1.0，gamma等于或过于接近1会有“Q值过大”的风险。一般选择0.99，在某些任务上需要调整。 而如果 gamma 选的很小，智能体只会关注短期回报，可能导致陷入局部最优。\nrollouts，num_envs # rollouts 对应 rl_games 中的 horizon_length\nrollouts： 与环境交互的次数，rollout 一次，可以得到 num_envs 条 transitions，每次 update 前，可以收集到 rollouts * num_envs 条 transitions。\n假设 rollouts=96, num_envs=128，那么每次 update 时的 buffer size 为 12288（~2^13.5）。因\n此，并行环境数量 num_envs 实际上也是一个我们需要去调整的超参数，该参数通常根据计算机性能来选择。先确定 num_envs，再确定 rollouts。\n为了尽可能准确地用多条轨迹去描述环境与策略的关系，在随机因素大的环境中，推荐增加 rollouts 的值以加大采样步数。\nmini_batches # mini_batches = rollouts * num_envs / batch_size，\n这里的 batch_size 对应 rl_games 中的 minibatch_size\n同理，\nbatch_size = rollouts * num_envs / mini_batches，\n假设 batch_size = 2**12(4096)，num_envs=128， rollouts=96，可以计算出 mini_batches=3\n假设 batch_size = 2**12(4096)，num_envs=128， rollouts=128，可以计算出 mini_batches=4\n对于 on-policy 算法，batch_size 推荐选取范围为（2**9 ~ 2**14），batch_size 越大，训练越慢，但更容易获得单调上升的学习曲线。\nlearning_epoch # 对应 rl_games 中的 mini_epochs\n对应【曾伊言】提到的数据复用次数 reuse_times（不是 update_times）。该参数一般设为 8，可以从偏小的数值开始尝试，以避免过拟合。\ntime_limit_bootstrap # 该参数使用在 skrl 中的 on-policy 算法上，默认为 False。\n在强化学习中，环境的 episode 可能因为时间限制而提前结束，而不是因为任务自然完成（如达到终止状态）。 如果将时间限制终止（truncation）视为失败并将最终奖励设为 0，会导致值函数低估这些状态的真实价值。 为了修正这个问题，time_limit_bootstrap 允许在 episode 因时间限制而终止时，用值函数的预测值来补偿最终奖励，从而减少不必要的值函数低估。 因此，建议将该值设置为 True\n# time-limit (truncation) bootstrapping if self._time_limit_bootstrap[uid]: rewards[uid] += self._discount_factor[uid] * values * truncated[uid] 参考博文 # 强化学习中的调参经验与编程技巧(on policy 篇) 曾伊言-深度强化学习调参技巧：以D3QN、TD3、PPO、SAC算法为例 【RLHF】怎样让 PPO 训练更稳定？早期人类征服 RLHF 的驯化经验 影响PPO算法性能的10个关键技巧（附PPO算法简洁Pytorch实现） ","date":"29 March 2023","externalUrl":null,"permalink":"/posts/algorithm/9-rl%E8%B0%83%E5%8F%82/","section":"Posts","summary":"","title":"以 skrl 库为例的 PPO 算法调参指南","type":"posts"},{"content":"","date":"29 January 2023","externalUrl":null,"permalink":"/authors/harlan-chou/","section":"Authors","summary":"","title":"Harlan-Chou","type":"authors"},{"content":" Marching Cubes 详解与实现 # 等值面是空间中的一张曲面，在该曲面上函数F(x，y，z)的值等于某一给定值。准确地讲，是指在某一网格空间中，假若每一结点保存着三变量函数F(x，y，z)，而且网格单元在x，y，z方向上的连续采样值为F(x，y，z)，则对于某一给定值Fi，等值面是由所有满足\n$S={(x，y，z) | F(x，y，z)=Fi}$\n的点组成的一张曲面。\n按照此严格定义下得到的等值面表达式如下：\n$F(x，y，z)=a_0+a_1x+a_2y+a_3z+a_4xy+a_5yz+a_6xz+a_7xyz$\nMarching Cubes（MC）算法是一种三维重建算法，常被用于三维重建任务，也在一些游戏中用于程序生成地形。简单来说，对于某个三维物体，例如下图所示的蓝色物体，由于直接提取该物体的等值面较为繁琐，MC算法意图使用近似的等值面对该物体重新建模，以简化等值面的提取。\n1. Cube Index # 首先，我们需要将该物体细分为一个个单元格（grid），每个单元格的各个顶点和边都用一个标量来索引。\n​ ​\n接下来，我们要判断各单元格的各顶点是否在3D几何体内。在覆盖物体外围的单元格中，物体的表面会与单元格相交，将单元格的某些顶点分隔在外面，我们用 cubeindex 来表示 8 个顶点的内外情况。\ncubeindex 包含 8 个 bit，分别表示单元格的 8 个顶点。假设第一个顶点在等值面的外面，我们令 cubeindex 与 1 作与运算，则 cubeindex 的第 1 个 bit 被置为 1。同理，若第二个顶点在等值面外，我们令 cubeindex 与 2 作与运算，2 的二进制表示为 10，则 cubeindex 的第 2 个 bit 被置为 1。\ncubeindex = 0; if (grid.val[0] \u0026lt; isolevel) cubeindex |= 1; if (grid.val[1] \u0026lt; isolevel) cubeindex |= 2; if (grid.val[2] \u0026lt; isolevel) cubeindex |= 4; if (grid.val[3] \u0026lt; isolevel) cubeindex |= 8; if (grid.val[4] \u0026lt; isolevel) cubeindex |= 16; if (grid.val[5] \u0026lt; isolevel) cubeindex |= 32; if (grid.val[6] \u0026lt; isolevel) cubeindex |= 64; if (grid.val[7] \u0026lt; isolevel) cubeindex |= 128; 其中，isoLevel​表示等值面（isolevel surface）的阈值。Marching Cubes是一种用于从三维体数据生成等值面的算法，等值面是指具有相同数值的体数据点构成的表面。isoLevel​定义了等值面所代表的数值。在算法中，如果一个体数据点的值低于isoLevel​，则该点被认为在等值面的外面；反之，如果一个体数据点的值高于或等于isoLevel​，则该点被认为在等值面内部。\n现在，我们以 Marching Square 为例，尝试重建一个圆出来。本例参考自[2]，图例为本人绘制。\n首先设定一些参数，圆的半径radius​，圆心在世界中的实际坐标center​。我们使用7*7的网格来进行重建，其中各轴上点的数量numPointPerAxis​为 8，那么各轴上 cube 的数量numVoxelPerAxis​为numPointPerAxis-1=7​个，id​ 表示各点在网格中的的索引坐标。另外设定每个 cube 的实际长度为cubeSize​。在本例中，我们将网格的中心与圆心对齐，这在后面的偏置计算中会用到。\n​ ​\n各顶点在网格中的实际坐标为 id * cubeSize​ ，网格中心(圆心)的实际坐标为 numPointsPerAxis * cubeSize * 0.5​ ，则该顶点相对圆心的偏置为：\nfloat3 offset = id * cubeSize - numPointsPerAxis * cubeSize * 0.5; 最终，我们可以得到这个点的位置和这个点对应的值。\nfloat3 pos = center + offset; float value = length(offset) - radius 最后，我们拿这个值与等值面的阈值 isolevel 进行比较，判断这个点在等值面内还是面外，并为相应的位值1或0。\n2. Cube Edge # 接下来，我们以上述运算完得到的 cubeindex 的十进制作为索引，找出在 edgeTable 中对应的一个十六进制数。这个十六进制数表示了单元格中 12 条边的被切割情况。\nint edgeTable[256]={ 0x0 , 0x109, 0x203, 0x30a, 0x406, 0x50f, 0x605, 0x70c, 0x80c, 0x905, 0xa0f, 0xb06, 0xc0a, 0xd03, 0xe09, 0xf00, 0x190, 0x99 , 0x393, 0x29a, 0x596, 0x49f, 0x795, 0x69c, 0x99c, 0x895, 0xb9f, 0xa96, 0xd9a, 0xc93, 0xf99, 0xe90, 0x230, 0x339, 0x33 , 0x13a, 0x636, 0x73f, 0x435, 0x53c, 0xa3c, 0xb35, 0x83f, 0x936, 0xe3a, 0xf33, 0xc39, 0xd30, 0x3a0, 0x2a9, 0x1a3, 0xaa , 0x7a6, 0x6af, 0x5a5, 0x4ac, 0xbac, 0xaa5, 0x9af, 0x8a6, 0xfaa, 0xea3, 0xda9, 0xca0, 0x460, 0x569, 0x663, 0x76a, 0x66 , 0x16f, 0x265, 0x36c, 0xc6c, 0xd65, 0xe6f, 0xf66, 0x86a, 0x963, 0xa69, 0xb60, 0x5f0, 0x4f9, 0x7f3, 0x6fa, 0x1f6, 0xff , 0x3f5, 0x2fc, 0xdfc, 0xcf5, 0xfff, 0xef6, 0x9fa, 0x8f3, 0xbf9, 0xaf0, 0x650, 0x759, 0x453, 0x55a, 0x256, 0x35f, 0x55 , 0x15c, 0xe5c, 0xf55, 0xc5f, 0xd56, 0xa5a, 0xb53, 0x859, 0x950, 0x7c0, 0x6c9, 0x5c3, 0x4ca, 0x3c6, 0x2cf, 0x1c5, 0xcc , 0xfcc, 0xec5, 0xdcf, 0xcc6, 0xbca, 0xac3, 0x9c9, 0x8c0, 0x8c0, 0x9c9, 0xac3, 0xbca, 0xcc6, 0xdcf, 0xec5, 0xfcc, 0xcc , 0x1c5, 0x2cf, 0x3c6, 0x4ca, 0x5c3, 0x6c9, 0x7c0, 0x950, 0x859, 0xb53, 0xa5a, 0xd56, 0xc5f, 0xf55, 0xe5c, 0x15c, 0x55 , 0x35f, 0x256, 0x55a, 0x453, 0x759, 0x650, 0xaf0, 0xbf9, 0x8f3, 0x9fa, 0xef6, 0xfff, 0xcf5, 0xdfc, 0x2fc, 0x3f5, 0xff , 0x1f6, 0x6fa, 0x7f3, 0x4f9, 0x5f0, 0xb60, 0xa69, 0x963, 0x86a, 0xf66, 0xe6f, 0xd65, 0xc6c, 0x36c, 0x265, 0x16f, 0x66 , 0x76a, 0x663, 0x569, 0x460, 0xca0, 0xda9, 0xea3, 0xfaa, 0x8a6, 0x9af, 0xaa5, 0xbac, 0x4ac, 0x5a5, 0x6af, 0x7a6, 0xaa , 0x1a3, 0x2a9, 0x3a0, 0xd30, 0xc39, 0xf33, 0xe3a, 0x936, 0x83f, 0xb35, 0xa3c, 0x53c, 0x435, 0x73f, 0x636, 0x13a, 0x33 , 0x339, 0x230, 0xe90, 0xf99, 0xc93, 0xd9a, 0xa96, 0xb9f, 0x895, 0x99c, 0x69c, 0x795, 0x49f, 0x596, 0x29a, 0x393, 0x99 , 0x190, 0xf00, 0xe09, 0xd03, 0xc0a, 0xb06, 0xa0f, 0x905, 0x80c, 0x70c, 0x605, 0x50f, 0x406, 0x30a, 0x203, 0x109, 0x0 }; 在如下面的左图所示，在一个单元格中，假设只有点 3 在等值面外，我们容易得到 cubeindex = 0000 1000 ，对应的十进制为 8。查找表 edgeTable 得到 edgeTable[8] = 1000 0000 1100，表示边 2,3, and 11 与等值面相交（如右图所示）。\n​ ​\n接下来，我们通过插值的方法找出来三个相交点的坐标 P：\n$P = P_1 + (isovalue - V_1) (P_2 - P_1) / (V_2 - V_1)$\nP1 和 P2 是相交边的两个顶点的坐标，如边2上的点 2 和 3。V1、V2 是这两个点在该 cube 中的值。\n另外，有人建议插值应按[3]所示处理，这解决了一个等值面上小裂缝的问题。\n3. Triangular Table # 最后，我们得到了 n 个坐标点，这些坐标点可以组成 m 个三角形，这就需要我们按照一定的顺序来连接各个顶点，可以通过查表 triTable 来获得。\nint triTable[256][16] = {{-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 1, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 8, 3, 9, 8, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, 1, 2, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 2, 10, 0, 2, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {2, 8, 3, 2, 10, 8, 10, 9, 8, -1, -1, -1, -1, -1, -1, -1}, {3, 11, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 11, 2, 8, 11, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 9, 0, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 11, 2, 1, 9, 11, 9, 8, 11, -1, -1, -1, -1, -1, -1, -1}, {3, 10, 1, 11, 10, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 10, 1, 0, 8, 10, 8, 11, 10, -1, -1, -1, -1, -1, -1, -1}, {3, 9, 0, 3, 11, 9, 11, 10, 9, -1, -1, -1, -1, -1, -1, -1}, {9, 8, 10, 10, 8, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 3, 0, 7, 3, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 1, 9, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 1, 9, 4, 7, 1, 7, 3, 1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 4, 7, 3, 0, 4, 1, 2, 10, -1, -1, -1, -1, -1, -1, -1}, {9, 2, 10, 9, 0, 2, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1}, {2, 10, 9, 2, 9, 7, 2, 7, 3, 7, 9, 4, -1, -1, -1, -1}, {8, 4, 7, 3, 11, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {11, 4, 7, 11, 2, 4, 2, 0, 4, -1, -1, -1, -1, -1, -1, -1}, {9, 0, 1, 8, 4, 7, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1}, {4, 7, 11, 9, 4, 11, 9, 11, 2, 9, 2, 1, -1, -1, -1, -1}, {3, 10, 1, 3, 11, 10, 7, 8, 4, -1, -1, -1, -1, -1, -1, -1}, {1, 11, 10, 1, 4, 11, 1, 0, 4, 7, 11, 4, -1, -1, -1, -1}, {4, 7, 8, 9, 0, 11, 9, 11, 10, 11, 0, 3, -1, -1, -1, -1}, {4, 7, 11, 4, 11, 9, 9, 11, 10, -1, -1, -1, -1, -1, -1, -1}, {9, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 5, 4, 0, 8, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 5, 4, 1, 5, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {8, 5, 4, 8, 3, 5, 3, 1, 5, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, 9, 5, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 0, 8, 1, 2, 10, 4, 9, 5, -1, -1, -1, -1, -1, -1, -1}, {5, 2, 10, 5, 4, 2, 4, 0, 2, -1, -1, -1, -1, -1, -1, -1}, {2, 10, 5, 3, 2, 5, 3, 5, 4, 3, 4, 8, -1, -1, -1, -1}, {9, 5, 4, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 11, 2, 0, 8, 11, 4, 9, 5, -1, -1, -1, -1, -1, -1, -1}, {0, 5, 4, 0, 1, 5, 2, 3, 11, -1, -1, -1, -1, -1, -1, -1}, {2, 1, 5, 2, 5, 8, 2, 8, 11, 4, 8, 5, -1, -1, -1, -1}, {10, 3, 11, 10, 1, 3, 9, 5, 4, -1, -1, -1, -1, -1, -1, -1}, {4, 9, 5, 0, 8, 1, 8, 10, 1, 8, 11, 10, -1, -1, -1, -1}, {5, 4, 0, 5, 0, 11, 5, 11, 10, 11, 0, 3, -1, -1, -1, -1}, {5, 4, 8, 5, 8, 10, 10, 8, 11, -1, -1, -1, -1, -1, -1, -1}, {9, 7, 8, 5, 7, 9, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 3, 0, 9, 5, 3, 5, 7, 3, -1, -1, -1, -1, -1, -1, -1}, {0, 7, 8, 0, 1, 7, 1, 5, 7, -1, -1, -1, -1, -1, -1, -1}, {1, 5, 3, 3, 5, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 7, 8, 9, 5, 7, 10, 1, 2, -1, -1, -1, -1, -1, -1, -1}, {10, 1, 2, 9, 5, 0, 5, 3, 0, 5, 7, 3, -1, -1, -1, -1}, {8, 0, 2, 8, 2, 5, 8, 5, 7, 10, 5, 2, -1, -1, -1, -1}, {2, 10, 5, 2, 5, 3, 3, 5, 7, -1, -1, -1, -1, -1, -1, -1}, {7, 9, 5, 7, 8, 9, 3, 11, 2, -1, -1, -1, -1, -1, -1, -1}, {9, 5, 7, 9, 7, 2, 9, 2, 0, 2, 7, 11, -1, -1, -1, -1}, {2, 3, 11, 0, 1, 8, 1, 7, 8, 1, 5, 7, -1, -1, -1, -1}, {11, 2, 1, 11, 1, 7, 7, 1, 5, -1, -1, -1, -1, -1, -1, -1}, {9, 5, 8, 8, 5, 7, 10, 1, 3, 10, 3, 11, -1, -1, -1, -1}, {5, 7, 0, 5, 0, 9, 7, 11, 0, 1, 0, 10, 11, 10, 0, -1}, {11, 10, 0, 11, 0, 3, 10, 5, 0, 8, 0, 7, 5, 7, 0, -1}, {11, 10, 5, 7, 11, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {10, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 0, 1, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 8, 3, 1, 9, 8, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1}, {1, 6, 5, 2, 6, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 6, 5, 1, 2, 6, 3, 0, 8, -1, -1, -1, -1, -1, -1, -1}, {9, 6, 5, 9, 0, 6, 0, 2, 6, -1, -1, -1, -1, -1, -1, -1}, {5, 9, 8, 5, 8, 2, 5, 2, 6, 3, 2, 8, -1, -1, -1, -1}, {2, 3, 11, 10, 6, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {11, 0, 8, 11, 2, 0, 10, 6, 5, -1, -1, -1, -1, -1, -1, -1}, {0, 1, 9, 2, 3, 11, 5, 10, 6, -1, -1, -1, -1, -1, -1, -1}, {5, 10, 6, 1, 9, 2, 9, 11, 2, 9, 8, 11, -1, -1, -1, -1}, {6, 3, 11, 6, 5, 3, 5, 1, 3, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 11, 0, 11, 5, 0, 5, 1, 5, 11, 6, -1, -1, -1, -1}, {3, 11, 6, 0, 3, 6, 0, 6, 5, 0, 5, 9, -1, -1, -1, -1}, {6, 5, 9, 6, 9, 11, 11, 9, 8, -1, -1, -1, -1, -1, -1, -1}, {5, 10, 6, 4, 7, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 3, 0, 4, 7, 3, 6, 5, 10, -1, -1, -1, -1, -1, -1, -1}, {1, 9, 0, 5, 10, 6, 8, 4, 7, -1, -1, -1, -1, -1, -1, -1}, {10, 6, 5, 1, 9, 7, 1, 7, 3, 7, 9, 4, -1, -1, -1, -1}, {6, 1, 2, 6, 5, 1, 4, 7, 8, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 5, 5, 2, 6, 3, 0, 4, 3, 4, 7, -1, -1, -1, -1}, {8, 4, 7, 9, 0, 5, 0, 6, 5, 0, 2, 6, -1, -1, -1, -1}, {7, 3, 9, 7, 9, 4, 3, 2, 9, 5, 9, 6, 2, 6, 9, -1}, {3, 11, 2, 7, 8, 4, 10, 6, 5, -1, -1, -1, -1, -1, -1, -1}, {5, 10, 6, 4, 7, 2, 4, 2, 0, 2, 7, 11, -1, -1, -1, -1}, {0, 1, 9, 4, 7, 8, 2, 3, 11, 5, 10, 6, -1, -1, -1, -1}, {9, 2, 1, 9, 11, 2, 9, 4, 11, 7, 11, 4, 5, 10, 6, -1}, {8, 4, 7, 3, 11, 5, 3, 5, 1, 5, 11, 6, -1, -1, -1, -1}, {5, 1, 11, 5, 11, 6, 1, 0, 11, 7, 11, 4, 0, 4, 11, -1}, {0, 5, 9, 0, 6, 5, 0, 3, 6, 11, 6, 3, 8, 4, 7, -1}, {6, 5, 9, 6, 9, 11, 4, 7, 9, 7, 11, 9, -1, -1, -1, -1}, {10, 4, 9, 6, 4, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 10, 6, 4, 9, 10, 0, 8, 3, -1, -1, -1, -1, -1, -1, -1}, {10, 0, 1, 10, 6, 0, 6, 4, 0, -1, -1, -1, -1, -1, -1, -1}, {8, 3, 1, 8, 1, 6, 8, 6, 4, 6, 1, 10, -1, -1, -1, -1}, {1, 4, 9, 1, 2, 4, 2, 6, 4, -1, -1, -1, -1, -1, -1, -1}, {3, 0, 8, 1, 2, 9, 2, 4, 9, 2, 6, 4, -1, -1, -1, -1}, {0, 2, 4, 4, 2, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {8, 3, 2, 8, 2, 4, 4, 2, 6, -1, -1, -1, -1, -1, -1, -1}, {10, 4, 9, 10, 6, 4, 11, 2, 3, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 2, 2, 8, 11, 4, 9, 10, 4, 10, 6, -1, -1, -1, -1}, {3, 11, 2, 0, 1, 6, 0, 6, 4, 6, 1, 10, -1, -1, -1, -1}, {6, 4, 1, 6, 1, 10, 4, 8, 1, 2, 1, 11, 8, 11, 1, -1}, {9, 6, 4, 9, 3, 6, 9, 1, 3, 11, 6, 3, -1, -1, -1, -1}, {8, 11, 1, 8, 1, 0, 11, 6, 1, 9, 1, 4, 6, 4, 1, -1}, {3, 11, 6, 3, 6, 0, 0, 6, 4, -1, -1, -1, -1, -1, -1, -1}, {6, 4, 8, 11, 6, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {7, 10, 6, 7, 8, 10, 8, 9, 10, -1, -1, -1, -1, -1, -1, -1}, {0, 7, 3, 0, 10, 7, 0, 9, 10, 6, 7, 10, -1, -1, -1, -1}, {10, 6, 7, 1, 10, 7, 1, 7, 8, 1, 8, 0, -1, -1, -1, -1}, {10, 6, 7, 10, 7, 1, 1, 7, 3, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 6, 1, 6, 8, 1, 8, 9, 8, 6, 7, -1, -1, -1, -1}, {2, 6, 9, 2, 9, 1, 6, 7, 9, 0, 9, 3, 7, 3, 9, -1}, {7, 8, 0, 7, 0, 6, 6, 0, 2, -1, -1, -1, -1, -1, -1, -1}, {7, 3, 2, 6, 7, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {2, 3, 11, 10, 6, 8, 10, 8, 9, 8, 6, 7, -1, -1, -1, -1}, {2, 0, 7, 2, 7, 11, 0, 9, 7, 6, 7, 10, 9, 10, 7, -1}, {1, 8, 0, 1, 7, 8, 1, 10, 7, 6, 7, 10, 2, 3, 11, -1}, {11, 2, 1, 11, 1, 7, 10, 6, 1, 6, 7, 1, -1, -1, -1, -1}, {8, 9, 6, 8, 6, 7, 9, 1, 6, 11, 6, 3, 1, 3, 6, -1}, {0, 9, 1, 11, 6, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {7, 8, 0, 7, 0, 6, 3, 11, 0, 11, 6, 0, -1, -1, -1, -1}, {7, 11, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {7, 6, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 0, 8, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 1, 9, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {8, 1, 9, 8, 3, 1, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1}, {10, 1, 2, 6, 11, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, 3, 0, 8, 6, 11, 7, -1, -1, -1, -1, -1, -1, -1}, {2, 9, 0, 2, 10, 9, 6, 11, 7, -1, -1, -1, -1, -1, -1, -1}, {6, 11, 7, 2, 10, 3, 10, 8, 3, 10, 9, 8, -1, -1, -1, -1}, {7, 2, 3, 6, 2, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {7, 0, 8, 7, 6, 0, 6, 2, 0, -1, -1, -1, -1, -1, -1, -1}, {2, 7, 6, 2, 3, 7, 0, 1, 9, -1, -1, -1, -1, -1, -1, -1}, {1, 6, 2, 1, 8, 6, 1, 9, 8, 8, 7, 6, -1, -1, -1, -1}, {10, 7, 6, 10, 1, 7, 1, 3, 7, -1, -1, -1, -1, -1, -1, -1}, {10, 7, 6, 1, 7, 10, 1, 8, 7, 1, 0, 8, -1, -1, -1, -1}, {0, 3, 7, 0, 7, 10, 0, 10, 9, 6, 10, 7, -1, -1, -1, -1}, {7, 6, 10, 7, 10, 8, 8, 10, 9, -1, -1, -1, -1, -1, -1, -1}, {6, 8, 4, 11, 8, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 6, 11, 3, 0, 6, 0, 4, 6, -1, -1, -1, -1, -1, -1, -1}, {8, 6, 11, 8, 4, 6, 9, 0, 1, -1, -1, -1, -1, -1, -1, -1}, {9, 4, 6, 9, 6, 3, 9, 3, 1, 11, 3, 6, -1, -1, -1, -1}, {6, 8, 4, 6, 11, 8, 2, 10, 1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, 3, 0, 11, 0, 6, 11, 0, 4, 6, -1, -1, -1, -1}, {4, 11, 8, 4, 6, 11, 0, 2, 9, 2, 10, 9, -1, -1, -1, -1}, {10, 9, 3, 10, 3, 2, 9, 4, 3, 11, 3, 6, 4, 6, 3, -1}, {8, 2, 3, 8, 4, 2, 4, 6, 2, -1, -1, -1, -1, -1, -1, -1}, {0, 4, 2, 4, 6, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 9, 0, 2, 3, 4, 2, 4, 6, 4, 3, 8, -1, -1, -1, -1}, {1, 9, 4, 1, 4, 2, 2, 4, 6, -1, -1, -1, -1, -1, -1, -1}, {8, 1, 3, 8, 6, 1, 8, 4, 6, 6, 10, 1, -1, -1, -1, -1}, {10, 1, 0, 10, 0, 6, 6, 0, 4, -1, -1, -1, -1, -1, -1, -1}, {4, 6, 3, 4, 3, 8, 6, 10, 3, 0, 3, 9, 10, 9, 3, -1}, {10, 9, 4, 6, 10, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 9, 5, 7, 6, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, 4, 9, 5, 11, 7, 6, -1, -1, -1, -1, -1, -1, -1}, {5, 0, 1, 5, 4, 0, 7, 6, 11, -1, -1, -1, -1, -1, -1, -1}, {11, 7, 6, 8, 3, 4, 3, 5, 4, 3, 1, 5, -1, -1, -1, -1}, {9, 5, 4, 10, 1, 2, 7, 6, 11, -1, -1, -1, -1, -1, -1, -1}, {6, 11, 7, 1, 2, 10, 0, 8, 3, 4, 9, 5, -1, -1, -1, -1}, {7, 6, 11, 5, 4, 10, 4, 2, 10, 4, 0, 2, -1, -1, -1, -1}, {3, 4, 8, 3, 5, 4, 3, 2, 5, 10, 5, 2, 11, 7, 6, -1}, {7, 2, 3, 7, 6, 2, 5, 4, 9, -1, -1, -1, -1, -1, -1, -1}, {9, 5, 4, 0, 8, 6, 0, 6, 2, 6, 8, 7, -1, -1, -1, -1}, {3, 6, 2, 3, 7, 6, 1, 5, 0, 5, 4, 0, -1, -1, -1, -1}, {6, 2, 8, 6, 8, 7, 2, 1, 8, 4, 8, 5, 1, 5, 8, -1}, {9, 5, 4, 10, 1, 6, 1, 7, 6, 1, 3, 7, -1, -1, -1, -1}, {1, 6, 10, 1, 7, 6, 1, 0, 7, 8, 7, 0, 9, 5, 4, -1}, {4, 0, 10, 4, 10, 5, 0, 3, 10, 6, 10, 7, 3, 7, 10, -1}, {7, 6, 10, 7, 10, 8, 5, 4, 10, 4, 8, 10, -1, -1, -1, -1}, {6, 9, 5, 6, 11, 9, 11, 8, 9, -1, -1, -1, -1, -1, -1, -1}, {3, 6, 11, 0, 6, 3, 0, 5, 6, 0, 9, 5, -1, -1, -1, -1}, {0, 11, 8, 0, 5, 11, 0, 1, 5, 5, 6, 11, -1, -1, -1, -1}, {6, 11, 3, 6, 3, 5, 5, 3, 1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 10, 9, 5, 11, 9, 11, 8, 11, 5, 6, -1, -1, -1, -1}, {0, 11, 3, 0, 6, 11, 0, 9, 6, 5, 6, 9, 1, 2, 10, -1}, {11, 8, 5, 11, 5, 6, 8, 0, 5, 10, 5, 2, 0, 2, 5, -1}, {6, 11, 3, 6, 3, 5, 2, 10, 3, 10, 5, 3, -1, -1, -1, -1}, {5, 8, 9, 5, 2, 8, 5, 6, 2, 3, 8, 2, -1, -1, -1, -1}, {9, 5, 6, 9, 6, 0, 0, 6, 2, -1, -1, -1, -1, -1, -1, -1}, {1, 5, 8, 1, 8, 0, 5, 6, 8, 3, 8, 2, 6, 2, 8, -1}, {1, 5, 6, 2, 1, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 3, 6, 1, 6, 10, 3, 8, 6, 5, 6, 9, 8, 9, 6, -1}, {10, 1, 0, 10, 0, 6, 9, 5, 0, 5, 6, 0, -1, -1, -1, -1}, {0, 3, 8, 5, 6, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {10, 5, 6, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {11, 5, 10, 7, 5, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {11, 5, 10, 11, 7, 5, 8, 3, 0, -1, -1, -1, -1, -1, -1, -1}, {5, 11, 7, 5, 10, 11, 1, 9, 0, -1, -1, -1, -1, -1, -1, -1}, {10, 7, 5, 10, 11, 7, 9, 8, 1, 8, 3, 1, -1, -1, -1, -1}, {11, 1, 2, 11, 7, 1, 7, 5, 1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, 1, 2, 7, 1, 7, 5, 7, 2, 11, -1, -1, -1, -1}, {9, 7, 5, 9, 2, 7, 9, 0, 2, 2, 11, 7, -1, -1, -1, -1}, {7, 5, 2, 7, 2, 11, 5, 9, 2, 3, 2, 8, 9, 8, 2, -1}, {2, 5, 10, 2, 3, 5, 3, 7, 5, -1, -1, -1, -1, -1, -1, -1}, {8, 2, 0, 8, 5, 2, 8, 7, 5, 10, 2, 5, -1, -1, -1, -1}, {9, 0, 1, 5, 10, 3, 5, 3, 7, 3, 10, 2, -1, -1, -1, -1}, {9, 8, 2, 9, 2, 1, 8, 7, 2, 10, 2, 5, 7, 5, 2, -1}, {1, 3, 5, 3, 7, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 7, 0, 7, 1, 1, 7, 5, -1, -1, -1, -1, -1, -1, -1}, {9, 0, 3, 9, 3, 5, 5, 3, 7, -1, -1, -1, -1, -1, -1, -1}, {9, 8, 7, 5, 9, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {5, 8, 4, 5, 10, 8, 10, 11, 8, -1, -1, -1, -1, -1, -1, -1}, {5, 0, 4, 5, 11, 0, 5, 10, 11, 11, 3, 0, -1, -1, -1, -1}, {0, 1, 9, 8, 4, 10, 8, 10, 11, 10, 4, 5, -1, -1, -1, -1}, {10, 11, 4, 10, 4, 5, 11, 3, 4, 9, 4, 1, 3, 1, 4, -1}, {2, 5, 1, 2, 8, 5, 2, 11, 8, 4, 5, 8, -1, -1, -1, -1}, {0, 4, 11, 0, 11, 3, 4, 5, 11, 2, 11, 1, 5, 1, 11, -1}, {0, 2, 5, 0, 5, 9, 2, 11, 5, 4, 5, 8, 11, 8, 5, -1}, {9, 4, 5, 2, 11, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {2, 5, 10, 3, 5, 2, 3, 4, 5, 3, 8, 4, -1, -1, -1, -1}, {5, 10, 2, 5, 2, 4, 4, 2, 0, -1, -1, -1, -1, -1, -1, -1}, {3, 10, 2, 3, 5, 10, 3, 8, 5, 4, 5, 8, 0, 1, 9, -1}, {5, 10, 2, 5, 2, 4, 1, 9, 2, 9, 4, 2, -1, -1, -1, -1}, {8, 4, 5, 8, 5, 3, 3, 5, 1, -1, -1, -1, -1, -1, -1, -1}, {0, 4, 5, 1, 0, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {8, 4, 5, 8, 5, 3, 9, 0, 5, 0, 3, 5, -1, -1, -1, -1}, {9, 4, 5, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 11, 7, 4, 9, 11, 9, 10, 11, -1, -1, -1, -1, -1, -1, -1}, {0, 8, 3, 4, 9, 7, 9, 11, 7, 9, 10, 11, -1, -1, -1, -1}, {1, 10, 11, 1, 11, 4, 1, 4, 0, 7, 4, 11, -1, -1, -1, -1}, {3, 1, 4, 3, 4, 8, 1, 10, 4, 7, 4, 11, 10, 11, 4, -1}, {4, 11, 7, 9, 11, 4, 9, 2, 11, 9, 1, 2, -1, -1, -1, -1}, {9, 7, 4, 9, 11, 7, 9, 1, 11, 2, 11, 1, 0, 8, 3, -1}, {11, 7, 4, 11, 4, 2, 2, 4, 0, -1, -1, -1, -1, -1, -1, -1}, {11, 7, 4, 11, 4, 2, 8, 3, 4, 3, 2, 4, -1, -1, -1, -1}, {2, 9, 10, 2, 7, 9, 2, 3, 7, 7, 4, 9, -1, -1, -1, -1}, {9, 10, 7, 9, 7, 4, 10, 2, 7, 8, 7, 0, 2, 0, 7, -1}, {3, 7, 10, 3, 10, 2, 7, 4, 10, 1, 10, 0, 4, 0, 10, -1}, {1, 10, 2, 8, 7, 4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 9, 1, 4, 1, 7, 7, 1, 3, -1, -1, -1, -1, -1, -1, -1}, {4, 9, 1, 4, 1, 7, 0, 8, 1, 8, 7, 1, -1, -1, -1, -1}, {4, 0, 3, 7, 4, 3, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {4, 8, 7, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {9, 10, 8, 10, 11, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 0, 9, 3, 9, 11, 11, 9, 10, -1, -1, -1, -1, -1, -1, -1}, {0, 1, 10, 0, 10, 8, 8, 10, 11, -1, -1, -1, -1, -1, -1, -1}, {3, 1, 10, 11, 3, 10, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 2, 11, 1, 11, 9, 9, 11, 8, -1, -1, -1, -1, -1, -1, -1}, {3, 0, 9, 3, 9, 11, 1, 2, 9, 2, 11, 9, -1, -1, -1, -1}, {0, 2, 11, 8, 0, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {3, 2, 11, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {2, 3, 8, 2, 8, 10, 10, 8, 9, -1, -1, -1, -1, -1, -1, -1}, {9, 10, 2, 0, 9, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {2, 3, 8, 2, 8, 10, 0, 1, 8, 1, 10, 8, -1, -1, -1, -1}, {1, 10, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {1, 3, 8, 9, 1, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 9, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {0, 3, 8, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}, {-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}}; 对于上面 cubeIndex=8​ 的例子，我们查找triTable[8][] = {3, 11, 2, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1}，3 表示编号为 3 的边，我们按 3-11-2 的顺序，将各边上的点连接起来。这样，就构成了一个三角形。\n网格分辨率 # 将值已知或可以在空间中任意地方插值的区域多边形化时，我们通常要控制网格的分辨率。可以根据所需的平滑度或可用于显示该表面的算力来生成等值线的粗略近似。下面是用不同网格大小生成的两个\u0026quot;bobby分子\u0026quot;。\n​ ​\n参考 # 原文：http://paulbourke.net/geometry/polygonise/\n原文视频解读：https://www.bilibili.com/video/BV1Ev411r7vx/\n[1] Marching Cubes (carleton.edu)\n[2] https://www.bilibili.com/video/BV1bW4y1675m\n[3] http://paulbourke.net/geometry/polygonise/interp.c\n推荐视频：https://www.bilibili.com/video/BV1yJ411r73v\n‍\n‍\n","date":"29 January 2023","externalUrl":null,"permalink":"/posts/algorithm/3-marching-cubes-1/","section":"Posts","summary":"","title":"Marching Cubes -1 原理介绍","type":"posts"},{"content":" 1. 前言 # 对于人类来说，最初的动作都是在婴儿时期模仿大人的行为习得的。而对机器人来说，想要让它学习人类动作，也需要从模仿开始。其实，平常的机器人示教操作，就是一个初步的模仿过程。我们通过人工示教，来引导机器人做出相应的动作，即技能的学习。\n当今，示教操作主要分为三类，分别是直接动觉示教（Kinematics Teaching）、遥操作示教（Teleoperation）和视频演示示教（Learning from Video）。下面使用机械臂来举例说明：\n直接动觉示教：手动拖动机械臂，让其记忆拖动点位 遥操作示教：使用手持设备或VR技术，远程操作机械臂，记忆过程中的数据流 视频演示示教：通过对视频的特征提取，让机械臂学习相应动作 其中，视频演示示教最为自然，但模仿难度更大，因为它属于间接示教，涉及到复杂的人机解耦问题，是近年来的研究热点，也是本篇文章主要的探讨方向。\n2. 模仿学习发展历程 # 下图是近年来模仿学习方法的演变过程，图片来自 OpenDILab 的分享https://www.bilibili.com/video/BV1AZ4y1e7WP。图中从上到下是模仿学习方法的主要发展过程，从左到右是某一方法在某一方面的深入。 ​ ​ 行为克隆（Behavior Clone，BC）：指专家做什么，智能体就做一模一样的事，与监督学习较为相似。但问题是，不管专家的行为是否合理或有用，智能体都会完全模仿专家的行为。\n逆强化学习（Inverse RL，IRL）：IRL 与 RL 相比，它没有奖励函数，只有环境和专家示范。在训练的时候，智能体可以与环境交互，但它得不到奖励，它的奖励函数必须从专家示范中反推出来（这也是 IRL 要解决的关键问题）。接下来，再使用 RL 的方法学习出最优演员。\n​ ​\n逆强化学习总体上可以归结为两类：基于最大边际的逆强化学习和基于概率模型的逆强化学习。\n最大边际逆强化学习（Maximum Margin IRL）：在早期，IRL 使用最大边际形式化的思想来反推奖励函数。其缺点是经常有很多不同的奖励函数导致相同的专家策略，在这种情况下，所学到的奖励函数往往具有随机的偏好。所以该方法无法解决歧义的问题，这里就不对该方法进行过多讨论。\n参考文章：\nhttps://zhuanlan.zhihu.com/p/26682811 https://zhuanlan.zhihu.com/p/26766494 https://zhuanlan.zhihu.com/p/30210839 最大熵逆强化学习（Maximum Entropy IRL，MaxEnt IRL）：为了解决最大边际逆强化学习中的多解问题，学术界把目光逐渐转变到基于概率模型的方法上，如最大熵逆强化学习、深度逆强化学习等。\n最大熵原理是指，在学习概率模型时，在所有满足约束的概率模型（分布）中，熵最大的模型是最好的模型。这是因为，通过熵最大所选取的模型，没有对未知分布做任何约束或假设。\n现在，基于现有的理论，我们已经能够生成较好的模型了，但目前仍存在一个问题，即训练的时候需要投入大量的专家示范供模型学习。\n参考文章：\n最大熵逆强化学习：https://zhuanlan.zhihu.com/p/91819689 最大熵逆强化学习实现：https://zhuanlan.zhihu.com/p/95465350 生成对抗模仿学习（Generative Adversarial Imitation Learning，GAIL）：该算法类似于 GAN 与 IRL 的结合，以 Maximum Causal Entropy IRL 作为研究的基础框架，解决了逆强化学习计算成本⾼，学习效率低下的问题。\n我们可以比较一下逆强化学习与生成对抗网络。如下图所示，在生成对抗网络里，我们有一些很好的图、一个生成器和一个判别器。一开始，生成器不知道要产生什么样的图，它就会乱画。判别器的工作是给画的图打分，专家画的图得高分，生成器画的图得低分。生成器会想办法让判别器也给它画的图打高分。整个过程与逆强化学习是一模一样的。专家画的图就是专家示范，生成器就是演员，演员与环境交互产生的轨迹其实就等价于生成器画的这些图。然后我们需要学习的奖励函数就相当于判别器。奖励函数要给专家示范打高分，给演员与环境交互的轨迹打低分。接下来，演员会想办法，从已经学习出的奖励函数中得到高分，然后迭代地循环下去。\n​ ​\n参考文章：\n原文地址：https://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf https://zhuanlan.zhihu.com/p/354572550 例子源自《Easy-RL》：https://github.com/datawhalechina/easy-rl 生成对抗模仿学习(Wasserstein GAIL，W-GAIL) ​：\n将 Wasserstein 距离 ​与 GAIL 结合，类似于 Wasserstein GAN，实现差异测量，即在训练过程中有一个像交叉熵、准确率这样的数值来指示训练的进程，这个数值越小代表 GAN 训练得越好，代表生成器产生的图像质量越高。\n参考文章：\nWasserstein GAN 中文：https://zhuanlan.zhihu.com/p/25071913 WGAN 原文：https://arxiv.org/abs/1701.07875 http://multimedia.whu.edu.cn/index.php?a=show\u0026amp;catid=72\u0026amp;id=195\u0026amp;lang=1 3. AI动作捕捉 # 从第一节中，我们总结了视频演示示教是机器模仿领域未来的主要发展方向。由于它属于间接示教，并且我们的专家数据通常来自于第三人称视角，而对于机器人来说，任务是在第一视角下进行的。所以，我们如何把第三人称的专家数据泛化到第一人称，以及对 2D 视频流中 3D 信息的实时特征提取，成为目前研究的主要问题。\n对于第三人称视角模仿学习，我们可以引入领域对抗训练（domain adversarial training）的概念。领域对抗训练也是一种生成对抗网络的技术。如图所示，我们希望有一个特征提取器，使两幅不同领域（即不同视角）的图像通过特征提取器后，无法分辨出图像来自哪一个领域（即使智能体在第三人称的时候与它在第一人称时的视角是一样的），就是把最重要的特征提取出来。\n​ ​\n无论使用哪种方法去处理，其实都是基于分层的思想去做的。首先是一个表征学习的过程，我们把视频流中的环境信息通过特征提取器提取出来。之后，再使用提取出的特征信息作为专家示范去学习。\n对于双臂机器人的轨迹规划问题，在这里提出了两个场景。\n场景一：动作捕捉。近年来，元宇宙的概念逐渐被炒得火热，虚拟主播（vtuber）这一行业也正在加速发展，实时动作捕捉技术自然成为虚拟现实领域当下的研究热点之一。其中，实时动捕主要分为两个研究方向：面部动捕与全身动捕，然而我们主要关注全身动捕的一部分——双臂动捕。\n实际上，我们所拍摄的人体动作视频数据，提取出的是人体的关节信息，如何把人体关节信息及其运动轨迹泛化到机器人身上，使机器人在模仿人类动作的同时，保持轨迹的平滑，是一个值得思考的问题。一种方法是，使用学习的方法去优化轨迹。\n​ ​ ​ ​\n场景二：物体交互。物体交互技术是基于动作捕捉的进一步发展，我把它分为三个方面：虚拟交互、现实交互和虚拟现实交互。虚拟交互是完全在虚拟场景中的交互，例如游戏或仿真软件内的人机交互等。现实交互是在真实场景中的交互，如机器人与真实物体的交互。虚拟现实交互是现实的人与虚拟的物品的交互，如 VR 中的力反馈效果。\n当然，这里只讨论双臂与物体的交互。在现实交互中，双臂相较于单臂，有更多的对象参与到交互中，因此具有更高维的动作-状态空间、更高范围的解。目前多数的双臂问题都使用经典的控制方法来解决，但很难用于泛化的环境中。因为在交互中，两臂与物体间会产生摩擦、粘附和变形，这些变化很难明确且精准地表示出来。那么一种有效的方法就是模仿学习，我们为机器人提供人类行为的演示，让机器人学习一个策略去模仿人类动作。\n​ ​ ​\n4. 展望 # 最后，我们可以将动作捕捉与物体交互结合到一起，让机器人学会许多的人类行为。也就是说，我们可以让机器人看大量的视频，令机器人学会视频中的动作，例如跳舞、做家务等等，从而解放大量的底层劳动者，因为模型是可以迁移和泛化的。或者，也可以用于科幻影视作品的摄制过程。但是，目前的主要问题集中在算法稳定性的提升上，将经典的控制算法与机器学习深度融合，提高算法的可解释性，是一个推荐的方向。\n‍\n","date":"29 January 2023","externalUrl":null,"permalink":"/posts/algorithm/2-overview-of-imitation-learning/","section":"Posts","summary":"","title":"探讨-协作双臂的动作模仿","type":"posts"},{"content":" Timoshenko Beam # ​elastica.modules​用于构建不同的仿真系统\nimport numpy as np # Import modules from elastica.modules import BaseSystemCollection, Constraints, Forcing, Damping # Import Cosserat Rod Class from elastica.rod.cosserat_rod import CosseratRod # Import Damping Class from elastica.dissipation import AnalyticalLinearDamper # Import Boundary Condition Classes from elastica.boundary_conditions import OneEndFixedRod, FreeRod from elastica.external_forces import EndpointForces # Import Timestepping Functions from elastica.timestepper.symplectic_steppers import PositionVerlet from elastica.timestepper import integrate 在这个例子中，杆的一端被固定住，令另一端受力。\nclass TimoshenkoBeamSimulator(BaseSystemCollection, Constraints, Forcing, Damping): pass timoshenko_sim = TimoshenkoBeamSimulator() 接下来，定义这个杆的各项属性，包括材料、几何形状等。\n# setting up test params # rod 中单元体 elements 的数量 n_elem = 100 density = 1000 nu = 1e-4 E = 1e6 # 弹性模量 # For shear modulus of 1e4, nu is 99! # 泊松比，一般不超过0.5，这里是为了让变形更明显 poisson_ratio = 99 shear_modulus = E / (poisson_ratio + 1.0) # 剪切系数 # 三维空间中的起始坐标 start = np.zeros((3,)) # rod 的朝向 direction = np.array([0.0, 0.0, 1.0]) normal = np.array([0.0, 1.0, 0.0]) base_length = 3.0 base_radius = 0.25 base_area = np.pi * base_radius ** 2 我们根据上述的参数构建出一根 rod，然后把它加入到一开始创建的仿真系统中\nshearable_rod = CosseratRod.straight_rod( n_elem, start, direction, normal, base_length, base_radius, density, 0.0, # internal damping constant, deprecated in v0.3.0 E, shear_modulus=shear_modulus, ) timoshenko_sim.append(shearable_rod) 添加阻尼 # We also need to define damping_constant​ and simulation time_step​ and pass in .using()​ method.\ndl = base_length / n_elem dt = 0.01 * dl timoshenko_sim.dampen(shearable_rod).using( AnalyticalLinearDamper, damping_constant=nu, time_step=dt, ) 添加边界条件 # 第一个约束是，固定杆一端的位置。We do this using the .constrain()option and theOneEndFixedRodboundary condition. We are modifying thetimoshenko_simsimulator toconstraintheshearable_rodobject using theOneEndFixedRod type of constraint. ​​\n我们还要定义杆的哪一个节点需要被约束.，可以通过节点的索引 constrained_position_idx​来实现，这里我们固定了第一个节点。为了防止杆绕固定节点旋转, 我们需要在两个节点之间约束一个元素，这固定了杆的方向. 我们通过约束元素的索引 constrained_director_idx​来实现。例如对于 position, 我们约束杆的第一个元素. Together, this contrains the position and orientation of the rod at the origin.\ntimoshenko_sim.constrain(shearable_rod).using( OneEndFixedRod, constrained_position_idx=(0,), constrained_director_idx=(0,) ) print(\u0026#34;One end of the rod is now fixed in place\u0026#34;) 第二个约束是对杆的末端施加一个力。我们想在d1方向施加一个负的力，同时加到杆的末端，可以通过指定origin_force 和end_force 实现。我们还想随着时间逐渐提高力的大小，通过指定ramp_up_time来改变，这防止了因力的不连续导致的错误。\norigin_force = np.array([0.0, 0.0, 0.0]) end_force = np.array([-10.0, 0.0, 0.0]) ramp_up_time = 5.0 timoshenko_sim.add_forcing_to(shearable_rod).using( EndpointForces, origin_force, end_force, ramp_up_time=ramp_up_time ) print(\u0026#34;Forces added to the rod\u0026#34;) 添加 Unshearable Rod # 为了比较 shearable rod 和 unshearable rod，我们再添加一个unshearable rod。我们改变 rod 的泊松比来让它 unshearable。对于真实的unshearable rod，泊松比通常为 -1.0，然而这会导致系统数值不稳定，所以我们采用 -0.85 的泊松比。\n# Start into the plane unshearable_start = np.array([0.0, -1.0, 0.0]) unshearable_rod = CosseratRod.straight_rod( n_elem, unshearable_start, direction, normal, base_length, base_radius, density, 0.0, # internal damping constant, deprecated in v0.3.0 E, # Unshearable rod needs G -\u0026gt; inf, which is achievable with a poisson ratio of -1.0 shear_modulus=E / (-0.85 + 1.0), ) timoshenko_sim.append(unshearable_rod) timoshenko_sim.dampen(unshearable_rod).using( AnalyticalLinearDamper, damping_constant=nu, time_step=dt, ) timoshenko_sim.constrain(unshearable_rod).using( OneEndFixedRod, constrained_position_idx=(0,), constrained_director_idx=(0,) ) timoshenko_sim.add_forcing_to(unshearable_rod).using( EndpointForces, origin_force, end_force, ramp_up_time=ramp_up_time ) print(\u0026#34;Unshearable rod set up\u0026#34;) 系统结束 # 现在我们以及添加完需要的rods及其边界条件到我们的系统中。最后，我们需要结束这个系统。这个操作将遍历系统，重新排列事物，并预计算有用的数值，为系统进行仿真做好准备。\ntimoshenko_sim.finalize() print(\u0026#34;System finalized\u0026#34;) 注意，如果在 finalize 后对 rod 做出了一些改变，需要 re-setup 该系统，即重新运行上述的所有代码。\n定义仿真时间 # 我们还要决定该仿真的运行时间，以及使用哪种时间步长方法。默认方法是 PositionVerlet 算法。这里我们仿真10s。\nfinal_time = 10.0 total_steps = int(final_time / dt) print(\u0026#34;Total steps to take\u0026#34;, total_steps) timestepper = PositionVerlet() 运行仿真 # 对于仿真的运行，我们结合 timoshenko_sim，使用 timestepper 方法，执行了 total_steps 步后，到达 final_time。\nintegrate(timestepper, timoshenko_sim, final_time, total_steps)\n‍\nPost Processing Results # 现在仿真已经结束，我们想处理仿真结果，我们将比较the solutions for the shearable and unshearable beams​与analytical Timoshenko and Euler-Bernoulli beam results. ​​\n# Compute beam position for sherable and unsherable beams. def analytical_result(arg_rod, arg_end_force, shearing=True, n_elem=500): base_length = np.sum(arg_rod.rest_lengths) # 在间隔 0.0 和 base_length 之间返回 n_elem 个均匀间隔的数据 # 即每个节点距离起始点的长度 arg_s = np.linspace(0.0, base_length, n_elem) if type(arg_end_force) is np.ndarray: acting_force = arg_end_force[np.nonzero(arg_end_force)] else: acting_force = arg_end_force acting_force = np.abs(acting_force) linear_prefactor = -acting_force / arg_rod.shear_matrix[0, 0, 0] quadratic_prefactor = ( -acting_force / 2.0 * np.sum(arg_rod.rest_lengths / arg_rod.bend_matrix[0, 0, 0]) ) cubic_prefactor = (acting_force / 6.0) / arg_rod.bend_matrix[0, 0, 0] if shearing: return ( arg_s, arg_s * linear_prefactor + arg_s ** 2 * quadratic_prefactor + arg_s ** 3 * cubic_prefactor, ) else: return arg_s, arg_s ** 2 * quadratic_prefactor + arg_s ** 3 * cubic_prefactor 现在，我们想去画出结果。首先需要指出的是，如何接收杆的位置，它们位于rod.position_collection[dim, n_elem]​。在本例中，我们画出 x- 和 z-轴。\ndef plot_timoshenko(shearable_rod, unshearable_rod, end_force): import matplotlib.pyplot as plt analytical_shearable_positon = analytical_result( shearable_rod, end_force, shearing=True ) analytical_unshearable_positon = analytical_result( unshearable_rod, end_force, shearing=False ) fig = plt.figure(figsize=(5, 4), frameon=True, dpi=150) ax = fig.add_subplot(111) ax.grid(b=True, which=\u0026#34;major\u0026#34;, color=\u0026#34;grey\u0026#34;, linestyle=\u0026#34;-\u0026#34;, linewidth=0.25) ax.plot( analytical_shearable_positon[0], analytical_shearable_positon[1], \u0026#34;k--\u0026#34;, label=\u0026#34;Timoshenko\u0026#34;, ) ax.plot( analytical_unshearable_positon[0], analytical_unshearable_positon[1], \u0026#34;k-.\u0026#34;, label=\u0026#34;Euler-Bernoulli\u0026#34;, ) ax.plot( shearable_rod.position_collection[2, :], shearable_rod.position_collection[0, :], \u0026#34;b-\u0026#34;, label=\u0026#34;n=\u0026#34; + str(shearable_rod.n_elems), ) ax.plot( unshearable_rod.position_collection[2, :], unshearable_rod.position_collection[0, :], \u0026#34;r-\u0026#34;, label=\u0026#34;n=\u0026#34; + str(unshearable_rod.n_elems), ) ax.legend(prop={\u0026#34;size\u0026#34;: 12}) ax.set_ylabel(\u0026#34;Y Position (m)\u0026#34;, fontsize=12) ax.set_xlabel(\u0026#34;X Position (m)\u0026#34;, fontsize=12) plt.show() plot_timoshenko(shearable_rod, unshearable_rod, end_force) ​ ‍​ ","date":"28 January 2023","externalUrl":null,"permalink":"/posts/algorithm/1-timoshenko-beam/","section":"Posts","summary":"","title":"基于 pyelastic 的悬臂梁仿真","type":"posts"},{"content":"","date":"13 June 2022","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"你好！我是周浩然，目前在小米机器人事业部担任灵巧手算法工程师。感兴趣的方向包括强化学习、灵巧操作等内容。\n欢迎与我联系，我的邮箱是 jou072@126.com\nMEng in Mechatronics Engineering, 2025\n上海理工大学, University of Shanghai for Science and Technology (USST) Experience # 上海市主动健康协同创新中心 2023/3 - 2023/9 派遣实习 国家自然基金项目, 咽拭子采样机器人。 上海卓易得机器人有限公司 2023/11 - 2024/3 人形机器人上肢算法实习生 上海傅里叶智能科技有限公司 2024/5 - 2024/8 机器人强化学习算法实习生 北京小米机器人科技有限公司 2024/8 - 2025/7 机器人算法实习生 北京小米机器人科技有限公司 2025/7 - now 机器人多自由度灵巧手操作算法工程师 Awards # 中国研究生机器人创新设计大赛 - 国三等奖 大学生工程训练与创新能力竞赛 - 国银奖 \u0026amp; 省特等奖 大学生机械创新竞赛 - 省一等奖 大学生电子设计竞赛 - 省一等奖 人工智能创新竞赛 - 省一等奖 Robocom 竞赛 - 省三等奖 Projects # NoneJou072/robopal robopal: a multi-platform, modular robot simulation framework based on MuJoCo, mainly used for reinforcement learning and control algorithm implementation of robotic arms. Python 152 15 NoneJou072/robochain A simulation framework based on ROS2 and LLMs(like GPT) for robot interaction tasks in the era of large models Python 110 12 Publications # Intelligent Redundant Manipulation for Long-Horizon Operations with Multiple Goal-conditioned Hierarchical Learning【 homepage 】\nHaoran Zhou, Xiankun Lin\nAdvanced Robitics(Journal), 2024 We propose a Multiple Goal-conditioned Hierarchical Learning (MGHL) framework to tackle long-horizon task manipulated by robotic arms.\nDyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception【 homepage 】\nHaoran Zhou, Yangwei You, Shuaijun Wang\nArxiv, 2025\nWe present DyDexHandover, a framework for dynamic bimanual handover, achieving human-like throwing and catching using RGB-only perception.\nCollaborative publications # The Development of a Visual Servo Simulator Based on ROS2 and MuJoCo\nYang Lu, Zongdao Li, Haoran Zhou, Liang Zhou\nIEEE International Conference on Robotics and Biomimetics (ROBIO), 2023\nDeep-reinforcement learning aided dynamic parameter identification of multi-joints manipulator\nZhuoran Bi, Wenlong Zhao, Yichao Huang, Haoran Zhou and Qingdu Li\nInternational Journal of Intelligent Systems Technologies and Applications, 2024\n","date":"1 January 1970","externalUrl":null,"permalink":"/about/","section":"homepage","summary":"","title":"About Me","type":"page"},{"content":"\u003c!DOCTYPE html\u003e Intelligent Redundant Manipulation for Long-Horizon Operations with Multiple Goal-conditioned Hierarchical Learning Intelligent Redundant Manipulation for Long-Horizon Operations with Multiple Goal-conditioned Hierarchical Learning Haoran Zhou, Xiankun Lin, Qingdu Li, University of Shanghai for Science and Technology Video Code Multiple Goal-conditioned Hierarchical Learning studies long-horizon manipulation tasks by learning several sub-tasks. Abstract Reinforcement Learning is a reasonable strategy to address object manipulation with long-horizon operation tasks. However, due to limitations in network capac- ity and exploration efficiency, it is challenging to achieve high success rates in dealing with such complex tasks. In this paper, we propose a Multiple Goal- conditioned Hierarchical Learning (MGHL) framework to tackle long-horizon task manipulated by redundant robotic arms. A long-horizon operation task is viewed as a sequential composition of multiple subtasks. By combining Multi- Task Learning and Hierarchical Reinforcement Learning, MGHL simultaneously learns multiple subtasks with sparse rewards under the same policy, and gradu- ally completes the goals of each subtask. During the pre-training of subtasks, a Mid-goal Guided Exploration Strategy is proposed to improve the sample qual- ity by establishing mid-goals to guide the exploration of the policies. In addition, MGHL implements a joint impedance controller that enables the robotic arm to take advantage of the flexibility of each joint to handle complex operations in a limited action space of the end effector. Three types of long-horizon manipulation tasks are introduced for training subtasks. The experimental results show that our proposed MGHL framework can learn multiple subtasks in the same policy with high success rate. By completing pre-trained subtasks one by one, MGHL outperforms several state-of-the-art algorithms on long-horizon tasks. Video Website template borrowed from NeRFies ","externalUrl":null,"permalink":"/mghl/","section":"homepage","summary":"","title":"","type":"page"},{"content":" 分治算法（Divide and Conquer）：字面上的解释是「分而治之」，就是把一个复杂的问题分成两个或更多的相同或相似的子问题，直到最后子问题可以简单的直接求解，原问题的解即子问题的解的合并。\n分治算法从实现方式上可以分为两种：「递归算法」和「迭代算法」。\n1.3 分治算法的适用条件 # 分治算法能够解决的问题，一般需要满足以下 个条件：\n可分解：原问题可以分解为若干个规模较小的相同子问题。 子问题可独立求解：分解出来的子问题可以独立求解，即子问题之间不包含公共的子子问题。 具有分解的终止条件：当问题的规模足够小时，能够用较简单的方法解决。 可合并：子问题的解可以合并为原问题的解，并且合并操作的复杂度不能太高，否则就无法起到减少算法总体复杂度的效果了。\n2. 分治算法的基本步骤 # 分解：把要解决的问题分解为成若干个规模较小、相对独立、与原问题形式相同的子问题。 求解：递归求解各个子问题。 合并：按照原问题的要求，将子问题的解逐层合并构成原问题的解。 其中第 1 步中将问题分解为若干个子问题时，最好使子问题的规模大致相同。换句话说，将一个问题分成大小相等的 k 个子问题的处理方法是行之有效的。在许多问题中，可以取 k=2。这种使子问题规模大致相等的做法是出自一种平衡子问题的思想，它几乎总是比子问题规模不等的做法要好。 其中第 2 步的「递归求解各个子问题」指的是按照同样的分治策略进行求解，即通过将这些子问题分解为更小的子子问题来进行求解。就这样一直分解下去，直到分解出来的子问题简单到只用常数操作时间即可解决为止。\n在完成第 2 步之后，最小子问题的解可用常数时间求得。然后我们再按照递归算法中回归过程的顺序，由底至上地将子问题的解合并起来，逐级上推就构成了原问题的解。\n按照分而治之的策略，在编写分治算法的代码时，也是按照上面的 3 个步骤来编写的，其对应的伪代码如下：\ndef divide_and_conquer(problems_n): # problems_n 为问题规模 if problems_n \u0026lt; d: # 当问题规模足够小时，直接解决该问题 return solove() # 直接求解 problems_k = divide(problems_n) # 将问题分解为 k 个相同形式的子问题 res = [0 for _ in range(k)] # res 用来保存 k 个子问题的解 for problem_k in problems_k: res[i] = divide_and_conquer(problem_k) # 递归的求解 k 个子问题 ans = merge(res) # 合并 k 个子问题的解 return ans # 返回原问题的解 参考：04.02.05 分治算法\n912. 排序数组 # 思路：\n我们使用归并排序来解决这个排序问题，用到了分治的思想。 class Solution { private: vector\u0026lt;int\u0026gt; tmp; void mergeSort(vector\u0026lt;int\u0026gt;\u0026amp; nums, int l, int r){ if(l \u0026gt;= r) return; int mid = (l + r) \u0026gt;\u0026gt; 1; mergeSort(nums, l, mid); mergeSort(nums, mid + 1, r); int i = l, j = mid + 1; int cnt = 0; while(i \u0026lt;= mid \u0026amp;\u0026amp; j \u0026lt;= r){ if(nums[i] \u0026lt;= nums[j]){ tmp[cnt++] = nums[i++]; } else{ tmp[cnt++] = nums[j++]; } } while(i \u0026lt;= mid){ tmp[cnt++] = nums[i++]; } while(j \u0026lt;= r){ tmp[cnt++] = nums[j++]; } for(int i = 0; i \u0026lt; r - l + 1; ++i){ nums[i + l] = tmp[i]; } } public: vector\u0026lt;int\u0026gt; sortArray(vector\u0026lt;int\u0026gt;\u0026amp; nums) { tmp.resize(nums.size(), 0); mergeSort(nums, 0, nums.size() - 1); return nums; } }; ","externalUrl":null,"permalink":"/posts/leetcode/datawhale%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%9F%BA%E7%A1%80%E8%AF%BE%E7%A8%8B%E7%AC%94%E8%AE%B0-%E5%88%86%E6%B2%BB/","section":"Posts","summary":"","title":"","type":"posts"},{"content":"github repo: LeRobot dataset V2.0 format\n","externalUrl":null,"permalink":"/posts/vla/1-lerobot/","section":"Posts","summary":"","title":"","type":"posts"},{"content":"","externalUrl":null,"permalink":"/en/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/en/","section":"Blowfish","summary":"","title":"Blowfish","type":"page"},{"content":"","externalUrl":null,"permalink":"/en/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"Dummy Second Author\u0026rsquo;s awesome dummy bio.\n","externalUrl":null,"permalink":"/authors/zhou/","section":"Authors","summary":"","title":"Haoran Zhou","type":"authors"},{"content":"","externalUrl":null,"permalink":"/en/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/en/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]